\chapter{Method}
%describe methods. One with similar background should be able to carry out the research based on this chapter.
\emph{The chapter covers strategies and methods used to achieve the objective of the project. Reasons for each choice of method or strategy are motivated and described in the sections.}

%% what methods are used to carry out the research. Describe that either several machine learning models are used and compared or one is used and the work involved improving it etc.

% Under rubriken ”Metod” anges vilken metod/metoder som använts för att få fram resultaten som presenteras i texten. Du bör också analysera valet av metod genom att motivera varför metoden fungerar bäst i sammanhanget. Beskrivningen av metod/metoder ska vara så noggranna och utförliga att gången i arbetsprocessen ska kunna upprepas.


\section{Choice of machine learning software}
	The author decided at an early stage to work with Scikit-learn, mainly due to the fact that the author has previous experience working with Python, and that it is claimed to be easy to use \cite{WEBSITE:27}. The plan was to stick with Scikit-learn, so long as it could be used to solve the aim of the thesis, and to change machine learning software when necessary. 

\section{Early-stage implementation and literature study} \label{sec:literature_study}
	In order to learn about the relevant machine learning theory to solve the thesis subtasks, the author deemed it necessary to learn both from experimenting in Scikit-learn in parallell to conducting a literature study. The first implementation was one where CART was used to classify the iris dataset (see \ref{sec:classification}). Eventually, the actual dataset from \ref{sec:provided_data} was used to see the effects of working with larger datasets and get an idea of how the different algorithms perform on the project subtasks. The dataset, being in six different Excel workbooks, was combined to one workbook.  It was discovered that using the full dataset was computationally heavy. Since a lot of experimenting was done in the beginning to learn concepts, either the iris dataset or data from one of the weather stations was mainly used in the beginning. Selecting subsets of the data to work with is known as sampling in statistics. Experimenting with a sample of the full dataset and/or the iris dataset was done throughout the project to perform learning experiments.

	It was originally planned to conduct a literature study for a duration of about four weeks in the early stages of the project, but learning relevant machine learning theory proved harder than estimated and as of such, the learning process continued throughout the project. A thesis objective could be established relatively early in the project, but finding the best means of achieving the objective proved hard. The following databases were used to find relevant books and academic papers to fulfill the aim of the study: Google Scholar, Google Books, Scopus and IEEE Xplore. At times, especially in the beginning of the project, it proved difficult for the author to machine learning concepts solely from academic sources. Therefore, non-peer-reviewed resources such as blogs and websites etc. were visited occasionally to attain a general understanding of key-concepts. If the author learned new concepts from non-peer-reviewed sources, its validity was checked in peer-reviewed material. A number of key-words were used to find relevant material in Google, Google Scholar, Google Books, Scopus and IEEE Xplore. Some of the keywords used:

	\begin{itemize}
		\item machine learning
		\item supervised learning
		\item multiclass classification
		\item regression
		\item cross-validation
		\item methodology
		\item overfitting
		\item performance
	\end{itemize}

	
\section{Data overview}
	The provided data described in \ref{sec:provided_data} was studied from an early stage in the project. This was done primarily to determine if the subtask target features should be solved by regression or classification algorithms, but also to study the data distribution of each target feature to reveal how often errors occurr, if the dataset is imbalanced etc. 

	 Although, what some may regard as a big dataset, was provided, the author was not sure if it was enough to fulfill the aim of the study. Furthermore, the provided dataset is geographically limited to a region in western Sweden, and the author doubted that an algorithm trained on this dataset could perform well in predicting/classifying observations from the northern parts in sweden for example. However, the weather stations which provided the data for this project appear to be the only ones that have both the DSC111 and DST111 sensors. As of such, it was ultimately decided to not investigate if data from additional weather stations could be added. When it comes to the size of the dataset, it was decided to see if the project subtasks can be solved using the given dataset, and to find additional data when needed.
	%look at the data provided
	%behövs mer data?  	%We also demonstrate that HOG templates have a relatively small effective capacity; one can train accurate HOG templates with 100–200 positive examples (rather than thousands of examples as is typically done \cite{ARTICLE:18}
	% Varför inte data från hela sverige: det är intressant att studera dst111 och dsc111, dessa finns endast tillsammans på denna sträcka => geografiskt avgränsad data



\section{Research approach}
		Among all of the algorithms in \ref{sec:supervised_algorithms}, finding the optimal ones that best solves the project subtasks may prove cumbersome. One approach could be to study the benefits and advantages of each algorithm or algorithm family in theory and predict which one is optimal for solving each of the project subtasks. In research approach theory, this is known as a deductive approach; where one makes a theoretical hypothesis and assumptions, and test and/or verify these empirically. Microsoft Azure and Scikit Learn provide intuitive guides of which specific algorithm to choose depending on size of dataset and which the type of problem \cite{WEBSITE:20, WEBSITE:21}. However similar algorithm-choosing guidelines seems to be few in academic resources. The author assumes that this is because performance are affected by a number of variables such as dataset size, amount of input features, choice of hyperparameters and more. 

	Instead of relying heavily on theory on which algorithm is optimal for a given situation, the author decided that a number of pre-determined algorithms should be tested on different situations. The author assumes that algorithms from the same algorithm familes, generally perform equally well. Thus if at least one algorithm from each algorithm family is represented, an optimal, or at least near-optimal, algorithm can be found that best solves each of the project subtasks. The process of comparing a number of algorithms, all of which are running on default settings to get a quick assessment, is termed "spot-checking" by \cite{WEBSITE:26}. It is deemed necessary in some situations by \cite{BOOK:6} to run multiple models on the same training and test dataset in parallel and compare their performances to choose an appropriate algorithm for the given problem domain. Including one additional algorithm for comparison in Scikit-learn means one extra line of code, which makes spot-checking possible with Scikit-learn. 

	\subsection{Choice of algorithms to evaluate}
		The algorithms to evaluate in spot-checking were chosen based on their interpretability, availability in Scikit-learn, ability to solve regression and classification problems, their general popularity and the need to balance the amount of parametric/non-parametric algorithms in total. 
	\begin{table}[H]
		\centering
		\caption{The supervised algorithms that are evaluated in this project. }
		\begin{tabular}[4]{l | l | l | l}
    			Name & Family & Solves & Parametric or non-parametric\\
			 \hline
			CART & Decision tree based learning & regression/classification problems & non-parametric \\
			kNN & Instance based learning & regression/classification problems & non-parametric \\
			Naïve Bayes & Bayesian learning & classification problems & parametric \\
			OLS & Regression based learning & regression problems & parametric \\
			Logistic regression & Regression based learning & classification problems & parametric \\
			Lasso & Regression based learning & regression problems & parametric \\
			Backpropagation & Deep learning & regression/classification problems & non-parametric \\
			Random forest & Ensemble learning & regression/classification problems & non-parametric
			\label{table:evaluated_algorithms}
		\end{tabular}
	\end{table}
		CART was chosen since it is the only decision tree based learning algorithm available in Scikit-learn. The author chose kNN, Naïve Bayes, Backpropagation and Random forest based on their ability to solve regression and classification problems, and based on a hypothesis made by the author from the information in \ref{sec:literature_study}, that they are among the most popular algorithms within their families. Three algorithms were chosen to represent regression based learning algorithms: OLS, Logistic regression and Lasso. OLS and Logistic regression were both chosen since they are parametric, generally popular and that either of them cannot solve both regression and classification problems. Lasso was chosen so that at least one regularization technique is evaulated. Lasso was chosen instead of Ridge or Elastic-net as regularization techniques since theory in \ref{sec:regularization} suggests a slight advantage to Lasso and Elastic-net, and Lasso was ultimately chosen because of its ability to perform feature selection and the fact that it has one hyperparameter instead of two. 

	Table \ref{table:evaluated_algorithms_regression} and \ref{table:evaluated_algorithms_classification} shows the algorithms used to solve regression and classification problems in this project. The algorithms are used with their default settings in spot-checking experiments.
	\begin{table}[H]
		\centering
		\caption{The supervised algorithms, which solves regression problems, that are evaluated in this project. Default settings for relevant hyperparameters are also shown.}
		\begin{tabular}[3]{l | c | c }
    			Name & Hyperparameter 1 default setting & Hyperparameter 2 default setting \\ 
			 \hline
			CART & splitting critera: gini &  \\ \hline
			kNN & $k = 5$ & distance metric: minkowski \\ \hline
			OLS &  & \\ \hline
			Lasso & $\lambda = 1$ & \\ \hline
			Backpropagation & n.o. hidden layers: 1 & n.o. hidden nodes: 100 \\ \hline
			Random forest & n.o. estimators: 10 &   
			\label{table:evaluated_algorithms_regression}
		\end{tabular}
	\end{table}

	\begin{table}[H]
		\centering
		\caption{The supervised algorithms, which solves classification problems, that are evaluated in this project. Default settings for relevant hyperparameters are also shown.}
		\begin{tabular}[3]{l | c | c }
    			Name & Hyperparameter 1 default setting & Hyperparameter 2 default setting \\ 
			 \hline
			CART & splitting critera: gini &  \\ \hline
			kNN & $k = 5$ & distance metric: minkowski \\ \hline
			Naïve Bayes &  & \\ \hline
			Logistic regression & penalty: $L_2$ & \\ \hline
			Backpropagation & n.o. hidden layers: 1 & n.o. hidden nodes: 100 \\ \hline
			Random forest & n.o. estimators: 10 &   
			\label{table:evaluated_algorithms_classification}
		\end{tabular}
	\end{table}


\section{Research strategy}
	%experimental approach, med det så testar vi ett antal algoritmer på default settings.
	An experimental research strategy was used to solve the subtasks of this project. The general goal of an experimental research strategy is to study the cause and effect relationship among variables in an experiment. The variable under consideration is called the independent variable, and the general idea is to study an overall effect of varying the independent variable.  


	\subsection{Experimental setups} \label{sec:exp_setups}
	To avoid repetitive explanation of experimental setups, and to keep some parameters constant throughout experiments, a number of experimental setups were set and named by the author of this project. They are as follows:

	\begin{itemize}
		\item{Holdout regression spot-checking: } This experiment tests all available regression algorithms in \ref{table:evaluated_algorithms_regression} where each algorithm runs with its default settings. 
			Holdout is used as a validation technique with which the dataset is split into 80\% training and 20\% testing. A random state is set 					 to shuffle the dataset before splitting, but to keep deterministic behavior. The random state is obtained by using a seed, $seed= 7$ is an arbitrary choice. 
		\item{Holdout classification spot-checking: } Same as the setup mentioned above but using the classification algorithms in \ref{table:evaluated_algorithms_classification}.
		\item{Cross-validation regression spot-checking: } Uses a similar setup as holdout regression spot-checking but with $k$-fold as a model validation 				technique instead of holdout. Although holdout was ultimately used as model validation technique in this project, the possibility of using $k$-				fold instead was investigated using this setup. A value of $k = 10$ is used since it is supported in \ref{sec:crossval}.
		\item{Cross-validation classification spot-checking: } Same as above apart from using stratified $k$-fold instead of regular $k$-fold and using the 				classification algorithms on default settings in \ref{table:evaluated_algorithms_classificationn}. Stratified $k$-fold is used to ensure that each class is equally represented in every fold. 
		\item{kNN optimization: } The purpose of this experiment is to find optimal values for $k$ in kNN. As brought up in \ref{sec:hyper_opt_techniques}, 				grid search is used as hyperparameter optimization technique, which applies to this experiment as well. Theory in \ref{sec:knn} suggests that 				there is no formula for providing an optimal value for $k$, and therefore an interval of values are tested, including the default value of $k=5$. 
			\begin{equation}
				k = 2^i \qquad \qquad 0 \leq i \leq 6
			\end{equation}
			A maximum of $i = 6$ is set to minimize the running time of this experiment. 
		\item{Backpropagation optimization: } This experiment deals with finding an optimal number of hidden nodes in Backpropagation. Similar to the choice of $k$ in kNN, an optimal number of hidden nodes in Backpropagation seems to be somewhat of a mystery. A starting point, as mentioned in \ref{sec:deep_learning}, is to set the number of hidden nodes $n_h$ to $n_h = \frac{2}{3}(n_i + n_o)$ where $n_i$ and $n_o$ are number of input and output nodes, which corresponds to input and target feature(s). The number of input features vary in this project, and as of such, the forementioned rule of thumb is used as a guidline to create a numeric interval to test different values for $n_h$.
			\begin{equation}
				n_h = 4^i \qquad \qquad 0 \leq i \leq 4
			\end{equation}
			The default value in Scikit-learn is $n_h = 100$, which is also included in the grid search.
		\item{Lasso optimization: } The purpose of this experiment is to find an optimal value for $\lambda$ in Lasso. Apart from the standard value $\lambda = 1$ a range of values are tested.
			\begin{equation}
				\lambda = \frac{100}{10i} \qquad \qquad 0 < i < 6
			\end{equation} 
	\end{itemize}

\section{Choice of model validation technique}
	From what the author learned from the literature review, it was planned to use a combination of holdout and $k$-fold as model validation technique: to perform an initial split of training/test data, use cross-validation to train a model and test it on the test data. It was believed that the combination of these techniques would yield high performance and generalization among the algorithms. However, it was discovered with using the built-in functionality in Scikit-learn to implement $k$-fold, it was not possible to train a model to be tested at a later point in Python code. This means it was possible to attain a cross-validation score from a given dataset, but to train a model using cross-validation and ultimately test it on a test dataset was not possible. The same reason is primarily why holdout was used as model validation technique instead of $k$-fold or stratified $k$-fold to achieve the final results. The way overfitting is detected in this project is to compare how a fitted model performs on its training dataset, in contrast to its performance on the test dataset. The author did not find a way to achieve this using the built-in $k$-fold or stratified $k$-fold in Scikit-learn. Another reason why holdout was used instead of $k$-fold or stratified $k$-fold was that it has shorter running-time. This is most likely because $k$-fold essentially performs $k$ model fits, whose performances are averaged when finished, while holdout does one model fit. Although $k$-fold was not used in achieving the final results, it was used in the early stages of the project and the data preparation process, since it was planned to be used throughout the project. 

	It was chosen to work with a value of $k= 10$ for both $k$-fold and stratified $k$-fold since this is supported in theory \ref{sec:crossval}. When cross-validation was used, stratified $k$-fold was used for classifying precipitation type to ensure that each class was equally represented in the folds, and regular $k$-fold was used for the other target features. If holdout was used as model validation technique, a 80\%\/\%20 training/test split was used. This is an arbitrary choice, taken from the fact that an optimal split does not appear to exist (see \ref{sec:supervisedlearning}).

\section{Choice of hyperparameter optimization technique} \label{sec:hyper_opt_techniques}
	Randomized search was not used in this project because it elicits randomness. Since an experimental approach used in this project, it is deemed necessary to maintain control over dependent variables and to ensure that several runs on the same settings produce the same results, i.e. the desired behavior of the spot-checking is to be deterministic. It was decided to use grid search instead, which is heavier computation-wise, but this was mitigated by optimizing three hyperparameters in total: $k$ in kNN, n.o. hidden neurons in Backpropagation and $\lambda$ in Lasso (see \ref{sec:delimitations}). %behöver kollas upp vad den heter exakt.

\section{Choice of performance measures}
	The author chose to use MSE to evaluate the performance of regression algorithms since this is supported in \ref{sec:regression}. Performance of classification algorithms were measured using macro $F1$ score (simply refererred to as $F1$ throughout the rest of this project). $F1$ was chosen due to the fact that it takes both precision and recall into account, and that using accuracy alone can be misleading for imbalanced datasets, which is the case of precipitation type. Accuracy was not used to evaluate classification algorithms, but still shown since it gives an intuititive idea of how an algorithm performs.

	As described in \ref{sec:objective}, the project subtasks are about finding the algorithm that performs best in terms of both performance and generalization. How performance is measured has already been covered in this section, generalization has not. As mentioned in \ref{sec:generalization}, overfitting is when a model fits its training data significantly better than its test data. Since theory in \ref{sec:generalization} suggests that underfitting is a result of a poor model fit, focus was set on overfitting in this project. It was decided to quantify overfitting rather than making it binary: yes overfitting or no overfitting. It was quantified by
	

\section{Experimental methodology}
	This section describes the methodology used to obtain the results of this project. The first step involves preparing the data which is followed up by obtaining the results to solve the project subtasks. 

	\subsection{Data preparation} \label{sec:method_dataprep}
		Pyle \cite{BOOK:15} argues that data preparation "is not a process which can be carried out blindly". He claims that an arbitrary dataset cannot be fixed by an automated data preparation process. This indicates that there is not one correct method to perform data preparation. The author selected methods from different academic sources \cite{BOOK:15, BOOK:16, BOOK:17} that are thought to be relevant steps in the data preparation process for this project. The process is done in the following way:
	\begin{enumerate}
		\item{Data cleaning: } It was revealed during the early stages of this project that some of the sensors elicited a considerable amount of errors. As mentioned in \ref{sec:delimitations}, this project is about modelling non-error behavior which is why all of the reported errors are removed at this step. There may also be suspected errors such as outliers that are removed at this step.
		\item{Feature selection: } The idea of this step is to see if any input features should be ruled out due to irrelevance or negative effect on overall performance.
		\item{Data transformation: } This step involves dealing with potential class imbalances and to see if input features can be transformed to improve overall performance.
	\end{enumerate}

	\subsection{Obtaining the results} \label{sec:method_results}
			After the data preparation process (see \ref{sec:method_dataprep}), the strategy used to find the best performance in predicting/classifying target feature $f_t$ in each subtask in this project is as follows:

		\begin{enumerate}
			\item Rank the possible input features $f_i$ to $f_t$. This is done by calculating a correlation score of $f_i$ to $f_t$. The correlation score is calculated with a so-called F-value which estimates the degree of linear dependency between two variables \cite{WEBSITE:28, WEBSITE:29}. 
			\item Run $i$ spot-checking experiments on the algorithms that can classify/predict $f_t$. The idea is to find which input features give the best performance- and generalization score for each algorithm. The $i$th spot-checking run uses the top $i$ input features.

				For regression tasks, holdout regression spot-checking experiments are used (see \ref{sec:exp_setups}). Performance is measured in MSE on the test dataset $MSE_{test}$. Overfitting $MSE_{diff}$ is measured by calculating the difference of how the model performs on the test dataset, in contrast to how it performs on the training dataset: $MSE_{diff} = MSE_{test} - MSE_{train}$. To attain an overall score which covers both performance and overfitting, an accumulated score $P_{acc} = MSE_{test} - MSE_{diff}$ is calculated.

				For classification problems, holdout classification spot-checking experiments are used (see \ref{sec:exp_setups}). Performance is measured in $F1$ macro average score on the test dataset $F1_{test}$. Overfitting $F1_{diff}$ is measured by calculating the difference of how the model performs on the test dataset in contrast to how it performs on the training dataset: $F1_{diff} = F1_{test} - F1_{train}$ . An overall score, which covers both performance and overfitting, is displayed as an accumulated score $P_{acc} = F1_{test} + F1_{diff}$. Accuracy is also displayed, but not evaluated on since it can be misleading in imbalanced datasets (see the classification section in literature study \ref{sec:classification}).

			\item Once an optimal set of input features are established for algorithm $a_k$ to predict/classify $f_t$, specific hyperparameter settings of $a_k$ are varied to see if its performance can be improved further. As mentioned in \ref{sec:delimitations}, only $k$ in kNN, number of hidden nodes in Backpropagation and $\lambda$ in Lasso are optimized. Lasso, Backpropagation and kNN optimization experiments are run as covered in \ref{sec:exp_setups}.
			\item Lastly, the optimal scores of each algorithm are listed and the algorithm, whose accumulated performance score is best, is identified as the top performing algorithm.
		\end{enumerate}
		

\section{Tools}
	A number of tools were used to achieve the results in this project:
	\begin{itemize}
		\item{Python: } Programming language.
		\item{Scikit-learn: } Python machine learning library. 
		\item{Pandas: } A Python library used in this project to read the dataset from Microsoft Excel workbooks.
		\item{Pyplot: } A Python library used to visualize datasets.
		\item{Microsoft Excel: } Microsoft software. The provided dataset was stored in separate Excel workbooks which is why built-in Excel tools were used to visualize the dataset and calculate mean values, number of occurrences etc.
		\item{Visual Basic: } A programming language that can be used in Microsoft Excel to create custom functionality and so-called macros. This was primarily used in the data cleaning and data transformation steps (see \ref{sec:datacleaning} and \ref{sec:transformation}) which would probably take a long to perform manually.
		\item{Sublime text: } Source code editor.
		\item{Git: } Version control tool. 
	\end{itemize}



