\chapter{Method}
%describe methods. One with similar background should be able to carry out the research based on this chapter.
\emph{The chapter covers strategies and methods used to achieve the objective of the project. Reasons for each choice of method or strategy are motivated in the sections.}

%% what methods are used to carry out the research. Describe that either several machine learning models are used and compared or one is used and the work involved improving it etc.

% Under rubriken ”Metod” anges vilken metod/metoder som använts för att få fram resultaten som presenteras i texten. Du bör också analysera valet av metod genom att motivera varför metoden fungerar bäst i sammanhanget. Beskrivningen av metod/metoder ska vara så noggranna och utförliga att gången i arbetsprocessen ska kunna upprepas.


\section{Choice of machine learning software}
	The author decided at an early stage to work with Scikit-learn, mainly due to the fact that the author has previous experience working with Python, and that Scikit-learn is claimed to be easy to use \cite{WEBSITE:27}. Scikit-learn have been used in peer-reviewed work, which is an indication of it being a legitimate software to use for such purposes \cite{ARTICLE:26, ARTICLE:27}.  

\section{Literature study} \label{sec:literature_study}
	%In order to learn about the relevant machine learning theory to solve the thesis subtasks, the author deemed it necessary to learn both from experimenting in Scikit-learn in parallell to conducting a literature study. The first implementation was one where CART was used to classify the iris dataset (see \ref{sec:classification}). Eventually, the actual dataset from \ref{sec:provided_data} was used to see the effects of working with larger datasets and get an idea of how the different algorithms perform on the project subtasks. The dataset, being in six different Excel workbooks, was combined to one workbook.  It was discovered that using the full dataset was computationally heavy. Since a lot of experimenting was done in the beginning to learn concepts, either the iris dataset or data from one of the weather stations was mainly used in the beginning. Selecting subsets of the data to work with is known as sampling in statistics. Experimenting with a sample of the full dataset and/or the iris dataset was done throughout the project to perform learning experiments.

	The following databases were used to find relevant books and academic papers to fulfill the aim of the study: Google Scholar, Google Books, Google, Scopus and IEEE Xplore. At times, especially in the beginning of the project, it proved difficult for the author to learn machine learning concepts solely from academic resources. Therefore, non-peer-reviewed resources such as blogs and websites etc. were visited occasionally. If the author learned new concepts from non-peer-reviewed sources, its validity was checked in peer-reviewed material. A number of key-words were used to find relevant material in. Some of the key-words used:

	\begin{itemize}
		\item machine learning
		\item supervised learning
		\item multiclass classification
		\item regression
		\item cross-validation
		\item methodology
		\item overfitting
		\item performance
		\item model
		\item sensors
		\item road condition
		\item road weather information system
	\end{itemize}

	The auhtor attempted to find related work in modelling RWIS sensors but was unable to do so.

	
\section{Data overview}
	The data provided by Trafikverket as seen in \ref{sec:provided_data} was studied from an early stage in the project. This was done primarily to determine if the subtask target features should be modelled by regression or classification algorithms, but also to study the data distribution of each target feature to reveal how often errors occur, if the dataset is imbalanced etc. 

	 Although what some may regard as a big dataset was provided, the author was not sure if it was big enough to fulfill the aim of the study. Furthermore, the provided dataset is geographically limited to a region in western Sweden, and the author doubted that any algorithm trained on its data could perform well in for example, predicting/classifying observations from the northern parts in Sweden. Furthermore, the RWIS stations which provided the data for this project appear to be the only ones in possession of Trafikverket that have both the DSC111 and DST111 sensors. As of such, it was ultimately decided to not investigate if data from additional RWIS stations should be added. When it comes to the size of the dataset, it was decided to see if the project subtasks can be solved using the given dataset, and to find additional data from the given weather stations if needed.
	%look at the data provided
	%behövs mer data?  	%We also demonstrate that HOG templates have a relatively small effective capacity; one can train accurate HOG templates with 100–200 positive examples (rather than thousands of examples as is typically done \cite{ARTICLE:18}
	% Varför inte data från hela sverige: det är intressant att studera dst111 och dsc111, dessa finns endast tillsammans på denna sträcka => geografiskt avgränsad data



\section{Research approach}
		Among all of the available supervised learning algorithms able to solve the project subtasks, finding optimal ones may prove cumbersome. One approach could be to study the benefits and advantages of each algorithm or algorithm family in theory, and assume which one is best for solving each of the project subtasks. In research approach theory, this is known as a deductive approach; where one makes theoretical hypothesis and assumptions, and test and/or verify these empirically. Microsoft Azure and Scikit Learn provide intuitive guides on which algorithm to choose depending on the dataset size and type of problem to be solved \cite{WEBSITE:20, WEBSITE:21}. However similar algorithm-choosing guidelines seem to be few in academic resources. The author assumes that this is because algorithm performance is affected by a number of variables such as dataset size, amount of input features, choice of hyperparameters and more. 

	Instead of relying heavily on theory on which algorithm is best for a given situation, the author decided that a number of pre-determined algorithms should be tested on different situations. The author assumes that algorithms from the same algorithm familes yield similar results. Thus, if at least one algorithm from each algorithm family is represented, well performing algorithms can be found that best solves each of the project subtasks. The process of comparing a number of algorithms, all of which are running on default settings to get a quick assessment, is termed "spot-checking" by \cite{WEBSITE:26}. It is deemed necessary in some situations by \cite{BOOK:6} to run multiple models on the same training and test dataset in parallel and compare their performances to choose an appropriate algorithm for a given problem. Including one additional algorithm for comparison in Scikit-learn means one extra line of code. This makes spot-checking possible with Scikit-learn. 

	\subsection{Choice of algorithms to evaluate}
		The algorithms to evaluate in this project were chosen based on their interpretability, availability in Scikit-learn, ability to solve regression and classification problems, their general popularity, need to represent a certain algorithm family and need to balance the amount of parametric/non-parametric algorithms in total. 
	\begin{table}[H]
		\centering
		\caption{The supervised algorithms that are evaluated in this project. }
		\resizebox{\textwidth}{!}{%
		\begin{tabular}[4]{l | l | l | l}
    			Name & Family & Solves & Parametric or non-parametric\\
			 \hline
			CART & Decision tree based learning & regression/classification problems & non-parametric \\
			kNN & Instance based learning & regression/classification problems & non-parametric \\
			Naïve Bayes & Bayesian learning & classification problems & parametric \\
			OLS & Regression based learning & regression problems & parametric \\
			Logistic regression & Regression based learning & classification problems & parametric \\
			Lasso & Regression based learning & regression problems & parametric \\
			MLP & Deep learning & regression/classification problems & non-parametric \\
			Random forest & Ensemble learning & regression/classification problems & non-parametric
			\label{table:evaluated_algorithms}
		\end{tabular}
		}
	\end{table}
		CART was chosen since it is the only decision tree based learning algorithm available in Scikit-learn. The author chose kNN, Naïve Bayes, MLP and Random forest based on their abilities to solve regression and classification problems, and based on a hypothesis made by the author that they are among the most popular algorithms within their families. Two algorithms were chosen to represent regression based learning algorithms: OLS and Logistic regression. OLS and Logistic regression were both chosen since they are parametric, generally popular and that either of them cannot solve both regression and classification problems. 

	Regularization techniques, which are covered in \ref{sec:regularization}, are used to penalize an algorithm to minimize overfitting, and in some cases perform feature selection. However, in Scikit-learn they can be used like a stand-alone algorithm: For example, the built-in Lasso regularization in Scikit-learn is basically a penalized version of OLS \cite{WEBSITE:31}. It was decide to use at least one regularization technique as a stand-alone algorithm in the spot-checking. Lasso was chosen instead of Ridge or Elastic-net as regularization techniques since theory in \ref{sec:regularization} suggests a slight advantage to Lasso and Elastic-net, and Lasso was ultimately chosen because of its ability to perform feature selection and the fact that it has one hyperparameter instead of two. This makes it easier to optimize hyperparameters with Lasso (see Lasso optimization in \ref{sec:exp_setups}). 

	Table \ref{table:evaluated_algorithms_regression} and \ref{table:evaluated_algorithms_classification} shows the algorithms used to solve regression- and classification problems in this project. The algorithms are used with their default Scikit-learn settings in spot-checking experiments.
	\begin{table}[H]
		\centering
		\caption{The supervised algorithms, which solves regression problems, that are evaluated in this project. Default settings for relevant hyperparameters are shown.}
		\begin{tabular}[3]{l | c | c }
    			Name & Hyperparameter 1 default setting & Hyperparameter 2 default setting \\ 
			 \hline
			CART & splitting critera: gini &  \\ \hline
			kNN & $k = 5$ & distance metric: minkowski \\ \hline
			OLS &  & \\ \hline
			Lasso & $\lambda = 1$ & \\ \hline
			MLP & n.o. hidden layers: 1 & n.o. hidden nodes: 100 \\ \hline
			Random forest & n.o. estimators: 10 &   
			\label{table:evaluated_algorithms_regression}
		\end{tabular}
	\end{table}

	\begin{table}[H]
		\centering
		\caption{The supervised algorithms, which solves classification problems, that are evaluated in this project. Default settings for relevant hyperparameters are shown.}
		\begin{tabular}[3]{l | c | c }
    			Name & Hyperparameter 1 default setting & Hyperparameter 2 default setting \\ 
			 \hline
			CART & splitting critera: gini &  \\ \hline
			kNN & $k = 5$ & distance metric: minkowski \\ \hline
			Naïve Bayes &  & \\ \hline
			Logistic regression & penalty: $L_2$ & \\ \hline
			MLP & n.o. hidden layers: 1 & n.o. hidden nodes: 100 \\ \hline
			Random forest & n.o. estimators: 10 &   
			\label{table:evaluated_algorithms_classification}
		\end{tabular}
	\end{table}


\section{Research strategy}
	%experimental approach, med det så testar vi ett antal algoritmer på default settings.
	An experimental research strategy was used to solve the subtasks of this project. The general goal of an experimental research strategy is to study the cause and effect relationship among variables in an experiment. The variable under consideration is called the independent variable, and the general idea is to study an overall effect of varying the independent variable.  


	\subsection{Experimental setups} \label{sec:exp_setups}
	To avoid repetitive explanation of experimental setups, and to keep some parameters constant throughout experiments, a number of experimental setups were set and named by the author of this project. They are as follows:

	\begin{itemize}
		\item{Holdout regression spot-checking: } This experiment tests all available regression algorithms in table \ref{table:evaluated_algorithms_regression} where each algorithm runs with its default settings. 
			Holdout is used as a validation technique with which the dataset is split into 80\% training and 20\% testing. A random state is set 					 to shuffle the dataset before splitting, but to keep deterministic behavior. The random state is obtained by using a seed, where $seed= 7$ is an arbitrary choice. 
		\item{Holdout classification spot-checking: } Same as the setup mentioned above but using the classification algorithms in table \ref{table:evaluated_algorithms_classification}.
		\item{Cross-validation regression spot-checking: } Uses a similar setup as holdout regression spot-checking but with $k$-fold as a model validation 				technique instead of holdout. A value of $k = 10$ is used since it is supported in theory from \ref{sec:crossval}.
		\item{Cross-validation classification spot-checking: } Similar to forementioned setup apart from using stratified $k$-fold instead of regular $k$-fold. The classification algorithms in table \ref{table:evaluated_algorithms_classification} are used. 
		\item{kNN optimization: } The purpose of this experiment is to find good values for $k$ in kNN. As brought up in \ref{sec:hyper_opt_techniques}, 				grid search is used as hyperparameter optimization technique, which applies to this experiment as well. Theory in \ref{sec:knn} suggests that 				there is no formula for providing an optimal value for $k$, and therefore an interval of values are tested, including the default value of $k=5$. 				The interval of values for $k$ is chosen based on generally popular choices found during the literature study by the author.
			\begin{equation}
				k = 2^i \qquad \qquad 0 \leq i \leq 6
			\end{equation}
			A maximum of $i = 6$ is set to minimize the running time of this experiment. 
		\item{MLP optimization: } This experiment deals with finding a good choice of number of hidden nodes in MLP. Similar to the choice of $k$ in kNN, an good choice of number of hidden nodes in MLP seems to be somewhat of a mystery. As mentioned in \ref{sec:deep_learning}, a starting point is to set the number of hidden nodes $n_h$ to $n_h = \frac{2}{3}(n_i + n_o)$ where $n_i$ and $n_o$ are number of input and output nodes respectively, which corresponds to input and target feature(s). The number of input features vary in this project, and as of such, the forementioned rule of thumb is used as a guidline to create a numeric interval to test different values for $n_h$.
			\begin{equation}
				n_h = 4^i \qquad \qquad 0 \leq i \leq 4
			\end{equation}
			The default value in Scikit-learn is $n_h = 100$, which is also included in the grid search.
		\item{Lasso optimization: } The purpose of this experiment is to find a good value for $\lambda$ in Lasso. Apart from the standard value $\lambda = 1$ a range of values are tested. The interval of values for $\lambda$ is chosen based on generally popular choices found during the literature study by the author.
			\begin{equation}
				\lambda = \frac{100}{10i} \qquad \qquad 0 < i < 6
			\end{equation} 
	\end{itemize}

\section{Choice of model validation technique}
	%From what the author learned from non peer-reviewed sources in the literature review, it was planned to use a combination of holdout and $k$-fold as model validation technique: to perform an initial split of training/test data, use cross-validation to train a model and test it on the test data. It was believed that the combination of these techniques would yield high performance and generalization among the algorithms. However, it was ultimately discovered during the literature study that the two techniques are most likely not used together, at least not in Scikit-learn, and that either one or the other of them is chosen as model validation technique. 

%used  with using the built-in functionality in Scikit-learn to implement $k$-fold, it was not possible to train a model to be tested at a later point in Python code. This means it was possible to attain a cross-validation score from a given dataset, but to train a model using cross-validation and ultimately test it on a test dataset was not possible. 

%The same reason is primarily why holdout was used as model validation technique instead of $k$-fold or stratified $k$-fold to achieve the final results. 
The way overfitting is detected in this project is to compare how a fitted model performs on the training dataset to its performance on the test dataset. The author did not find a way to achieve this using the built-in $k$-fold or stratified $k$-fold in Scikit-learn, which is one of the main reasons why holdout was preferred as model validation technique over $k$-fold. Another reason why holdout was used instead of $k$-fold, or stratified $k$-fold, was that it has shorter running-time. This is most likely because $k$-fold essentially performs $k$ model fits, whose performances are averaged when finished, while holdout does one model fit. Although $k$-fold was not used in achieving the final results, it was used in the early stages of the project, and during the data preparation process, due to the fact that it was originally planned to be used to attain the project results. 

	When $k$-fold was used, a value of $k= 10$ for both $k$-fold and stratified $k$-fold was used since this is supported in theory on cross-validation (see \ref{sec:crossval}). If $k$-fold was used, stratified $k$-fold was used for classifying precipitation type to ensure that each class was equally represented in the folds, and regular $k$-fold was used for regression problems. When holdout was used as model validation technique, a 80\%/\%20 training/test split was used. This is an arbitrary choice, taken from the assumption that an optimal split does not seem to exist in theory (see theory on holdout   \ref{sec:supervisedlearning}).

\section{Choice of hyperparameter optimization technique} \label{sec:hyper_opt_techniques}
	Randomized search was not used in this project because it elicits randomness. Since an experimental approach was used in this project, it was deemed necessary to maintain control over dependent variables, and to ensure that several runs on the same settings produce the same results, i.e. the desired behavior of the spot-checking is to be deterministic. It was decided to use grid search, which may be more demanding computation-wise than Randomized search. This was mitigated by limiting the interval of values to test, and to optimize three hyperparameters in total: $k$ in kNN, n.o. hidden neurons in MLP and $\lambda$ in Lasso (see project delimitations \ref{sec:delimitations}).

\section{Choice of performance measures}
	The author chose to use MSE to evaluate the performance of regression algorithms since theory on regression performance measure suggests that it is essential to use for regression problems (see \ref{sec:regression}). Performance of classification algorithms were measured using $\overline{F1}$ score. It was chosen due to the fact that it takes both precision and recall into account, and that theory in \ref{sec:classification} suggests that using accuracy alone can be misleading for imbalanced datasets, which is the case of precipitation type. Accuracy was not used to evaluate classification algorithms, but displayed nevertheless, since it can give an intuititive understanding of how an algorithm performs.

	As described in \ref{sec:objective}, the project subtasks are about finding algorithms that performs best in terms of both performance and generalization. How performance is measured has already been covered in this section, whereas generalization has not. As theory suggests in \ref{sec:generalization}, overfitting is when a model fits its training data significantly better than its test data. Theory from \ref{sec:generalization} indicate that underfitting is the result of a poor model fit. The author hypothesize that underfit models are ruled out as poor performers in this project, which is why focus was set on overcoming overfitting. It was decided to quantify overfitting rather than making it binary: overfitting/no overfitting. It was quantified by calculating the difference of how a model performs on its test dataset in contrast to its training dataset. Higher values in equation \ref{eq:overfitting} indicate overfitting.

	\begin{equation} \label{eq:overfitting}
		\mbox{Overfitting} = P_{test} - P_{train}
	\end{equation}
	

\section{Experimental methodology}
	This section describes the methodology used to obtain the results of this project. The first step involves preparing the data, which is followed up by obtaining the results to solve the project subtasks. 

	\subsection{Data preparation} \label{sec:method_dataprep}
		Pyle \cite{BOOK:15} argues that data preparation "is not a process which can be carried out blindly". He claims that an arbitrary dataset cannot be fixed by an automated data preparation process. This indicates that there is not one correct way to carry data preparation process. The author selected methods from three academic resources \cite{BOOK:15, BOOK:16, BOOK:17} that are thought to be relevant steps in the data preparation process for this project. The process was done in the following way:
	\begin{enumerate}
		\item{Data cleaning: } It was revealed during the early stages of this project that some of the sensors elicited a considerable amount of errors. As mentioned in the project delimitations \ref{sec:delimitations}, this project is about modelling non-error behavior, which is why any reported errors are removed at this step. There may also be suspected errors such as outliers that should be removed.
		\item{Feature selection: } The idea of this step is to see if any input features should be ruled out due to irrelevance or negative effect on overall performance. Suspected features which are believed to have a negative effect on overall performance are tested. %It was found in \ref{} that timestamp have a negative effect on overall performance.
		\item{Data transformation: } This step involves dealing with potential class imbalances and to see if feature engineering can be employed to improve overall performance. Oversampling techniques (see \ref{sec:imbalancedtheory}) are tested to see if they can be used to solve imbalanced datasets.
	\end{enumerate}

	\subsection{Obtaining the results} \label{sec:method_results}
			After the data preparation process (see \ref{sec:method_dataprep}), the strategy used to find the best performance in predicting/classifying target feature $f_t$ in each subtask in this project is as follows:

		\begin{enumerate}
			\item Rank the possible input feature $f_i$ to $f_t$. This is done by calculating a correlation score with $f_i$ to $f_t$. The correlation score is calculated with a so-called F-value which estimates the degree of linear dependency between two variables \cite{WEBSITE:28, WEBSITE:29}. The purpose of this step is to avoid testing all possible combinations of input features in finding a good choice of input features to use in the spot-checking step. This method does not capture non-linear relationships between $f_i$ and $f_t$. 

			\item Run $n$ spot-checking experiments on the algorithms that can classify/predict $f_t$, where $n$ is the amount of input features available in total. The idea is to find which input features give the best performance- and generalization score for each algorithm. The $i$th spot-checking run uses the top $i$ input features, where $1 \leq i \leq n$.

				For regression tasks, holdout regression spot-checking experiments are used (see \ref{sec:exp_setups}). Performance is measured in MSE on the test dataset $MSE_{test}$. Overfitting $MSE_{diff}$ is measured by calculating the difference of how a model performs on the test dataset, in contrast to how it performs on the training dataset: $MSE_{diff} = MSE_{test} - MSE_{train}$. To attain an overall score which covers both performance and overfitting, an accumulated score $P_{acc} = MSE_{test} + MSE_{diff}$ is calculated. A minimal score of $P_{acc}$ is desired for regression tasks.

				For classification problems, holdout classification spot-checking experiments are used (see \ref{sec:exp_setups}). Performance is measured in $\overline{F1}$ score on the test dataset $\overline{F1}_{test}$. Overfitting $\overline{F1}_{diff}$ is measured by calculating the difference of how the model performs on the test dataset in contrast to how it performs on the training dataset: $\overline{F1}_{diff} = \overline{F1}_{test} - \overline{F1}_{train}$. An overall score, which covers both performance and overfitting, is displayed as an accumulated score $P_{acc} = \overline{F1}_{test} - \overline{F1}_{diff}$. While a minimum $P_{acc}$ score is desired for regression tasks, the opposite is true for classification: a higher $P_{acc}$ scores means a better score. Accuracy is also displayed, but not evaluated on.

			\item Once a desired set of input features are established for algorithm $a_k$ to predict/classify $f_t$, specific hyperparameter settings of $a_k$ are varied to see if its performance can be improved further. As mentioned in the project delimitations \ref{sec:delimitations}, only $k$ in kNN, number of hidden nodes in MLP and $\lambda$ in Lasso are optimized. Lasso, MLP and kNN optimization experiments are run as described in \ref{sec:exp_setups}.
			\item Lastly, the optimized scores of each algorithm are listed. The algorithm, whose accumulated performance score is best, is identified as the top performing algorithm in predicting/classifying $f_t$.
		\end{enumerate}
		

\section{Tools}
	A number of tools were used to achieve the results in this project:
	\begin{itemize}
		\item{Python: } Programming language.
		\item{Scikit-learn: } Python machine learning library. 
		\item{Pandas: } A Python library used in this project to read the dataset from Microsoft Excel workbooks.
		\item{Pyplot: } A Python library used to visualize datasets.
		\item{Imbalanced-learn:} A python library with functionality to handle imbalanced datasets.
		\item{Microsoft Excel: } Microsoft software. The provided dataset was stored in separate Excel workbooks which is why built-in Excel tools were used to visualize the dataset and calculate mean values, number of occurrences etc.
		\item{Visual Basic: } A programming language that can be used in Microsoft Excel to create custom functionality and so-called macros. This was primarily used in the data cleaning and data transformation steps (see \ref{sec:datacleaning} and \ref{sec:transformation}) which would probably take a long to perform manually.
		\item{Sublime text: } Source code editor.
		\item{Git: } Version control tool. 
	\end{itemize}



