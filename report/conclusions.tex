\chapter{Discussion and conclusions}
\emph{This chapter presents a dicsussion on the results obtained, conclusions connected to the aim of this project and recommendations for Trafikverket.}

\section{Discussion}

	It is up to Trafikverket to decide what a reasonable margin for error is in stating that a model can or cannot model the behavior of a sensor. But when comparing the results from the regression subtasks in this project, modelling DST111 road surface temperature shows a higher error rate than that of precipitation amount and TIRS road surface temperature. Since DST111 and TIRS both measure road surface temperature, it is assumed that the model for DST111 generally performs poorly and as of such, the behavior of DST111 is hard to model with the given input features. Since any algorithm which models TIRS in this project can use DST111 road surface temperature as an input feature, which proves to have strong linear correlation to TIRS, the author assumes that if TIRS road surface temperature could be used as input feature to model DST111, a better performance could be obtained. However, it could also be that DST111 elicits less predictable behavior than TIRS: As reported in the data cleaning process (see \ref{sec:datacleaning}), measurements made by DST111 were significantly higher than TIRS during a brief period and those observations were therefore removed. 

	Two holdout regression spot-checking experiments was carried out to see the importance of having either TIRS- or DST111 road surface temperature to predict the other. The first experiment is to predict TIRS road surface temperature but to not use its top relevant input feature: DST111 road surface temperature. The second experiment is to predict DST111 road surface temperature allowing TIRS road surface temperature to be used as input feature. In this project, top performance for predicting TIRS road surface temperature was $MSE_{test} = 0.88$, this was when DST111 was allowed as input feature. When DST111 was not used, the best performance among algorithms to predict TIRS road surface temperature was Random forest with a reduced performance of $MSE_{test}=15.36$. In the case of predicting DST111 road surface temperature, it was not allowed to use TIRS road surface temperature as input feature in this project, resulting in a performance score of $MSE_{test}=10.16$. When TIRS road surface temperature was allowed as input feature, the best performing algorithm to predict DST111 road surface temperature was MLP with a performance score of $MSE_{test}=0.77$. These experiments indicate that the success of predicting road surface temperature of either of the two sensors depend on having a strongly linearly correlated input feature.

	As for the classification task of classifying precipitation type, the author deems that its performance should be evaluated on its $F1$ score rather than on its accuracy score. The recall scores from table \ref{table:classreport_prectype} show that the best model for modelling precipitation type performed well in predicting no precipitation: 90\% while the other precipitation types had significantly lower recall scores, the lowest being rain and snow mixed which was correctly classified in 3\% of its occurrences. The relatively low $F1$ score may be due to the fact that the dataset is imbalanced in terms of precipitation type, and that the imbalanced data problem was not successfully handled in this project. 

	In hindsight, the author thinks that the accumulated performance score presented in \ref{sec:method_results} can be improved by not allowing negative values in the overfitting score. An overfitting score of zero means that the model performs equally well on both the training and test dataset. A negative overfitting score means a model performed better on the test dataset than on its training dataset. The author hypothesize that a negative overfitting score is not necessarily better than one near zero. A model with a negative overfitting score increases its accumulated performance score. This may result in a non-optimal choice of algorithm having highest accumulated performance score, and thus being identified as the top performer for a given problem.

	Input features were ranked in relevancy to a given target feature based on linear dependency. Non-linear relationships between features were thus not captured. The results of this project may be improved either by using a different measurement for feature correlations, or to skip the ranking procedure and perform experiments on every combination of input feature.

	%In this project, the author chose to work with holdout as model validation technique, and the results are evaluated on the test dataset. However, the test dataset was also used to optimize algorithms hyperparameters. 
	It was learned at a late point by the author in this project that information from the test dataset from using holdout was not necessarily previously unseen for any algorithm involved. Hyperparameters were optimized to minimize error in test data performance. This means optimized values for hyperparameters were set for an algorithm based on its performance on the test dataset. If an independent validation dataset was used instead to optimize hyperparameters, information would not "leak" from the test dataset to achieve a high performance. The author recommends the reader to take note of this fact when reviewing the results of this project. 
	
	%Why separate test and validation sets? The error rate estimate of the final model on validation data will be biased (smaller than the true error rate) since the validation set is used to select the final model After assessing the final model on the test set, YOU MUST NOT tune the model any further!

\section{Conclusions}
	The aim of this project was to find optimized supervised learning algorithms to model the behavior of three sensors: Optic Eye, TIRS and DST111. This was desired so that Trafikverket can potentially replace existing sensors to reduce costs, or use as backup in case of failing sensors. The sensors make four different type of measurements in total: precipitation type, precipitation amount, TIRS road surface temperature and DST111 temperature. The objective was broken down into four subtasks which aimed at finding optimized algorithms to model each of the measurements of the sensors.

	The results obtained in this project indicate that the measurements made by Optic Eye: precipitation type and precipitation amount, are best modelled using CART for precipitation type and kNN for precipitation amount. An accuracy score of 0.84 and $\overline{F1}$ score of 0.46 were obtained in modelling precipitation type using Scikit-learn default settings, and the following input features: road surface condition, road friction, DST111 road surface temperature, hour and month. 	The $\overline{F1}$ score in classifying precipitation type can likely be improved by addressing its imbalanced dataset representation. Precipitation amount was best modelled using kNN, obtaining a performance score $MSE=0.54$ with $k=64$ using road friction as the only input feature. 

	The two remaining sensors measuring road surface temperature: TIRS and DST111, were best modelled using MLP and Random forest respectively. MLP was set to use 64 hidden nodes, with which a performance score $MSE=0.88$ was obtained in modelling TIRS road surface temperature using the following input features: DST111 road surface temperature, road surface condition, road friction, month, hour and precipitation type. As for modelling DST111 road surface temperature, the best model was obtained from using Random forest on Scikit-learn default settings with a performance score $MSE=10.16$ using the following input features: road surface condition, road friction, month, precipitation type and hour.

	The success of predicting either TIRS- or DST111 road surface temperature relies on having at least one strongly linearly correlated input feature, for instance: a different road surface temperature feature. The accumulated performance score used in this project to evaluate algorithms, in terms of test performance and generalization, is not an optimal evaluation score. It can be improved by not allowing negative overfitting scores in equation \ref{eq:overfitting}. If there are non-linear correlations among the different features, the overall results of this project can likely be improved by testing all possible combination of input features when modelling a given target feature, instead of using an input feature ranking method based on linear correlation.
%%skriv om features ranking

\section{Recommendations}
		The author recommends Trafikverket to refrain from modelling DST111 road surface temperature using the given algorithm and algorithm settings, but to investigate the possibility of modelling TIRS road surface temperature and precipitation amount using the algorithms and algorithm settings seen in table \ref{table:results_summary}. Given that all precipitation types are equally important to classify, the author recommends Trafikverket to investigate if the imbalanced data problem of precipitation type can be solved, either by collecting more data from the less represented classes, or to see if the problem can be solved using similar techniques as the ones used in this project (see attempts in handling class imbalance in \ref{sec:class_imbalance}). 