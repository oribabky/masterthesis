\chapter{Discussion and conclusions}
\emph{This chapter presents a dicsussion on the results obtained, and the methods used. It is followed up with conclusions connected to the aim of this project and ultimately recommendations for Trafikverket.}

\section{Discussion}
	The first part of this section covers a discussion on the results obtained, and the methods used to obtain the results are discussed in the second part.

	\subsection{Results}
	It is up to Trafikverket to decide what a reasonable margin for error is in stating that a model can or cannot model the behavior of a sensor, but when comparing the results from the regression subtasks in this project, modelling DST111 road surface temperature shows a higher error rate than that of precipitation amount and TIRS road surface temperature. The results from the experiments in section \ref{sec:results_dst111} reveals the importance of using data from a different road surface temperature sensor $a$ as input feature when modelling a road surface temperature sensor $b$. The results indicate that the modelling of either of the two sensors (DST111 or TIRS) is improved when doing so. This suggests that using a strongly linearly correlated input feature, such as a different road surface temperature sensor, improves the model significantly. Given that TIRS road surface temperature was not allowed to be used as input feature to predict DST111 road surface temperature, alternative input features may be needed to build a decent model. Road surface temperature is not the only type of temperature the RWIS measure, they also measure air temperature. The author speculates that air temperature have a linear correlation to road surface temperature, and could therefore likely improve the results of modelling DST111 road surface temperature if it is used as input feature. 

	As for the classification task of classifying precipitation type, the author deems that its performance should be evaluated on its $\overline{F1}$ score rather than on its accuracy score. The recall scores from table \ref{table:classreport_prectype} show that the best model for modelling precipitation type performed well in predicting no precipitation: 90\% while the other precipitation types had significantly lower recall scores, the lowest being rain and snow mixed which was correctly classified in 3\% of its occurrences. The relatively low $\overline{F1}$ score may be due to the fact that the dataset is imbalanced in terms of precipitation type, and that the imbalanced data problem was not successfully handled in this project. 

	
	In this project, optimization of hyperparameters was done based on an algorithm's performance on the test dataset. This means that information from the test dataset was not necessarily previously unseen for any algorithm involved. If an independent validation dataset was used instead to optimize hyperparameters, information would not "leak" from the test dataset in order to achieve a high performance. The author recommend readers to take note of this fact when reviewing the results of this project. 

	\subsection{Method}

	The method for calculating an accumulated performance score presented in section \ref{sec:method_results} could possibly be improved by not allowing negative values in the overfitting score. An overfitting score of zero means that the model performs equally well on both the training and test dataset. A negative overfitting score means a model performed better on the test dataset than on its training dataset. The author thinks that a negative overfitting score is not necessarily better than one close to zero. A model with a negative overfitting score increases its accumulated performance score. This may result in a non-optimal choice of algorithm having the best accumulated performance score, and thus being identified as the top performer for a given problem.

	In this project, input features were ranked in relevancy to a given target feature based on the degree of linear correlation. Non-linear relationships between features, that may or may not present a small degree of linear relationship, did not have as big an impact as a strong linear relationship has on the feature ranking. If there are non-linear relationships among features, feature ranking based primarily on linear correlations may not have been optimal. One way could be to test all possible combinations of features instead of relying on an input feature ranking. However, that would require much more time than varying the amount of top relevant input features. Equation \ref{eq:combinations_features} shows how many combinations there are with six input features, which is the same amount of input features used to predict TIRS road surface temperature.
\begin{equation} \label{eq:combinations_features}
	c(n= 6) = \sum_{r=1}^{6} \frac{6!}{r!(6-r)!} = 63
\end{equation} 
	The author thinks that feature ranking was suitable for this project since it's much less time consuming than to test dozens of combinations of input features for each subtask. 

	In this project it was decided to combine the data provided by the six weather stations along state road E6 in Sweden. The distance between the two stations that are furthest away from each other is approximately 140 kilometers \cite{WEBSITE:35}. An alternative approach could be to keep the data separated in six datasets, one per RWIS, and find optimized models for each RWIS. The author hypothesize that this could work if data provided by each station is big enough for training, and if differences in weather conditions among the weather stations are significant, so that it is beneficial to train models from local data alone. Another approach could be to keep the data combined, but to add a feature that shows which RWIS an observation is taken from. This could capture local weather condition patterns, while being able to learn from other stations as well, which could prove useful. For example, in the case of abnormal weather conditions for one station $S_1$, such weather conditions might be frequently occurring for another station $S_2$, in which case $S_1$ can learn from $S_2$. This approach could also potentially benefit from having a larger dataset overall, and to avoid the effort in training models separately.

	%In this project, the author chose to work with holdout as model validation technique, and the results are evaluated on the test dataset. However, the test dataset was also used to optimize algorithms hyperparameters. 

	
	%Why separate test and validation sets? The error rate estimate of the final model on validation data will be biased (smaller than the true error rate) since the validation set is used to select the final model After assessing the final model on the test set, YOU MUST NOT tune the model any further!

\section{Conclusions}
	The aim of this project was to find optimized supervised learning algorithms to model the behavior of three sensors: Optic Eye, Track Ice Road Sensor (TIRS) and DST111. This was desired so that Trafikverket can potentially replace existing sensors to reduce costs, or use as backup in case of failing sensors. The sensors make four different type of measurements in total: precipitation type, precipitation amount, TIRS road surface temperature and DST111 road surface temperature. The objective was broken down into four subtasks which aimed at finding optimized algorithms to model each of the measurements made by the sensors.

	The results obtained in this project indicate that the measurements made by Optic Eye: precipitation type and precipitation amount, are best modelled using Classification And Regression Tree (CART) for precipitation type and $k$-nearest neighbor (kNN) for precipitation amount. An accuracy score of 0.84 and $\overline{F1}$ score of 0.46 was obtained in modelling precipitation type using Scikit-learn default settings, and the following input features: road surface condition, road friction, DST111 road surface temperature, hour and month. 	The $\overline{F1}$ score in classifying precipitation type can likely be improved by addressing its imbalanced dataset representation. Precipitation amount was best modelled using kNN, obtaining a performance score $MSE=0.31$ with $k=64$ using road friction as the only input feature. 

	The two remaining sensors measuring road surface temperature: TIRS and DST111, were best modelled using Multi-Layer Perceptron (MLP) and Random forest respectively. MLP was set to use 64 hidden nodes, with which a performance score $MSE=0.88$ was obtained in modelling TIRS road surface temperature using the following input features: DST111 road surface temperature, road surface condition, road friction, month, hour and precipitation type. As for modelling DST111 road surface temperature, the best model was obtained from using Random forest on Scikit-learn default settings with a performance score of $MSE=10.16$ using the following input features: road surface condition, road friction, month, precipitation type and hour. The success of predicting either TIRS- or DST111 road surface temperature relies on having at least one strongly linearly correlated input feature, for instance: a different road surface temperature feature. 

	Apart from finding answers to the project subtasks, the author made some findings along the way:

	\begin{itemize}
		\item The accumulated performance score used in this project to evaluate algorithms, in terms of test performance and generalization, is not an optimal evaluation score. It can be improved by not allowing negative overfitting scores in equation \ref{eq:overfitting}. 
		\item If there are non-linear correlations among the different features, the overall results of this project can possibly be improved by testing all possible combinations of input features when modelling a given target feature, instead of using an input feature ranking method where linear correlations have the biggest impact. However, such testing requires more time, and as of such the author recommends to test all input features on low-dimensional datasets, and to consider input feature ranking for high-dimensional datasets. Any practicer considering input feature ranking should also consider if the ranking should focus on the degree of linear correlation, or some other type of measurement.
	\end{itemize}


\section{Recommendations}
		Even though it is up to Trafikverket to decide when a model performs sufficientlly well in modelling a target feature, the author recommends Trafikverket to refrain from modelling DST111 road surface temperature using the given algorithm, algorithm settings and input features since it has relatively bad performance when compared to the performance of modelling TIRS road surface temperature. Trafikverket should investigate the possibility of modelling TIRS road surface temperature and precipitation amount using the algorithms and algorithm settings seen in table \ref{table:results_summary}. However, Trafikverket should experiment with other input features from the RWIS as well, especially air temperature could potentially improve the performance of modelling DST111 road surface temperature. 

Given that all precipitation types are equally important to classify, the author recommends Trafikverket to investigate if the imbalanced data problem of precipitation type can be solved. This can be solved either by collecting more data from the less represented classes, or to use similar techniques as the ones used in this project (see attempts in handling class imbalance in section \ref{sec:class_imbalance}). 

	The dataset used in this project is limited to a region in western Sweden along state road E6. The author recommends Trafikverket to investigate if models can benefit from training on data from as many RWIS as possible, or if training with data from stations that are geographically close proves a better choice. If the observations from several RWIS are combined to one dataset, Trafikverket should experiment with having a feature stating which RWIS the observation is taken from.