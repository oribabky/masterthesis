\chapter{Literature Review} \label {ch:theory}
%Write about the theory used in the research.
\emph{The chapter gives both general and specific information on theory used in this project. It starts of with a general description of supervised learning, followed generalization and why it is important. It follows up with some information on supervised learning algorithms, how algorithm performance can be improved and lastly how to prepare datasets for applied machine learning.}


\section{Supervised learning} \label{sec:supervisedlearning}
	In supervised learning, the algorithm receives a dataset of labeled observations which are used to build a mathematical used to predict correct values for unseen data. A database table storing weather-related data could for example have thousands of database records (observations) where data in each record belong to certain database column headers (features) such as wind speed $w_s$, wind direction $w_d$ and time $t$. The goal of supervised learning is to build a model
\begin{equation} \label{eq:mappingfunction}
	y = f_{map}(x)
\end{equation}
such that when new input data $x_{new}$ is used, $f_{map}$ can predict $y_{new}$. The model is built from a dataset which is typically split into three parts:

\begin{itemize}
	\item {Training dataset:} Used to fit the model.
	\item {Validation dataset:} Used to give an unbiased evaluation of a model built from the training dataset which can be used potentially update its parameters in order to improve performance \cite{BOOK:6}.  %säg att i sklearn så finns inga validation sets? verkar det som? varken för cross val eller holdout 
	\item {Test dataset:} Gives an unbiased evaluation of the final model.
\end{itemize}
	%called "holdout" not lock box?
	Skocik et al. \cite{ARTICLE:20} call this a lock box approach. According to \cite{BOOK:6}, the proportions of the split is usually 60\% training, 30\% test and 10\% evaluation while \cite{ARTICLE:4} suggests that a common approach is 50\% training, 30\% validation and 20\% test. Success has also been shown by using 90\% of the data as training data \cite{ARTICLE:19}. It is suggested by \cite{ARTICLE:4} to employ the lock box approach in any machine learning project. 

	Supervised learning can be thought of as having a teacher supervising the algorithm. The correct answers are in the training data and the algorithm learns from being corrected by the teacher. Going back to the forementioned example of the weather station to give a brief example of how a supervised machine learning algorithm works: Suppose a training, validation and test dataset is provided and one wishes to predict wind speed $y = w_s$ based on wind direction and time $x = [x_1, x_2] = [w_d, t]$. During the training process, a supervised learning algorithm goes through the training dataset to build a model, as seen in Eq. \ref{eq:mappingfunction}, and possibly updated when validated against the validation dataset. Suppose the supervised learning algorithm used is Ordinary least squares (see section \ref{sec:reg_based_learning}) and a model is built from the training process: 

\begin{equation} \label{eq:example_ws}
	w_s = f_{map}([w_d, t]) = \beta_0 + \beta_1 w_d + \beta_2 t = 4 + 0.2w_d + 1.7t
\end{equation}
	The model can then be tested with the test dataset to see how it performs on unseen data.  %say that ols is interpretable and easy model, compared to say BP.

	Estimation of continous output variables, such as wind speed in the example presented above, is a regression problem. In supervised learning there are also algorithms associated with the problem of classification, which deals with categorizing data.


	\subsection{Classification predictive modeling} \label{sec:classification}
	In a classification problem, the computer is asked to place a new observation into one of $k$ categories (classes), $k \geq 2$ \cite{BOOK:1}. The problem of classifying new email as spam or not spam is an example of a classification problem. Google claims that their machine learning models can detect spam and phishing messages with 99.9\% accuracy in their widely used Gmail application \cite{WEBSITE:4}. Classification on two classes, as in the forementioned example, is known as binary classification and problems with three or more classes to be classified is called multiclass classification \cite{MISC:1}. Classifying precipation type, which is done in this project, is a multiclass classification problem since it has more than two classes.

	Another example of a classification problem, one that may well be the first that machine learning novices encounter, is classification of the Iris flower dataset. The dataset consists of 50 observations with four features: length and width of the sepals and petals, in centimeters. Based on this information, the problem is to classify an observation into one of three classes: Setosa, Versicolour, Virginica \cite{WEBSITE:5}.
	How the classification is carried out depends on the algorithm used to build the model. These kind of algorithms are commonly known as classifiers. There are several classifiers that can be used for the Iris dataset, but their performance in doing so may differ. Performance of classifiers are typically measured in terms of accuracy, which is the amount of correct predictions divided by the number of observations in the test dataset.
%Ta upp true positives, false positives, true negatives, false negatives? och skriv om ekvationerna!

\begin{equation}
	\mbox{accuracy} = \frac{\mbox{\#correct predictions total}}{\mbox{\#observations}}
\end{equation}
	
	A high accuracy score such as 90\% may seem promising, but what if 90\% of the test data is made up of Setosa alone? That means that the model correctly classified all Setosa observations, but failed on all of the Versicolour and Virginica classifications. This is probably an indication of an imbalanced dataset, which means that the different classes in the dataset are not equally represented. In these cases, bla \ref{BOOK:11} claims that classification accuracy is not a good metric to evaluate a model, and that other metrics such as precision-recall break-even, area under the curve etc. should be used instead. None of the performance metrics mentioned by \cite{BOOK:11} is available in Sckit-learn, but there is a performance metric available there known as macro-average $F_1$ that is suitable for evaluating imbalanced multiclass classification problems \cite{WEBSITE:25}. It averages the performance of classifying each individual class, in terms of precision and recall score. Equations \ref{eq:recall}, \ref{eq:precision} and \ref{eq:f1} shows how recall, precision and $F_1$ scores are calculated for a multiclass classification problem on one if its classes $A$. 

\begin{equation} \label{eq:recall}
	\mbox{Recall}(A) = \frac{\mbox{\#correct predictions} A}{\mbox{\#observations} A}
\end{equation}

\begin{equation} \label{eq:precision}
	\mbox{Precision}(A) = \frac{\mbox{\#correct predictions} A}{\mbox{\#identified occurrences} A}
\end{equation}

\begin{equation} \label{eq:f1}
	F_1(A) = 2 \cdot \frac{\mbox{Recall}(A) \cdot \mbox{Precision}(A)}{\mbox{Recall}(A) + \mbox{Precision}(A)}
\end{equation}

Another way to see if a high accuracy score is misleading is to analyze a so-called Confusion matrix. It shows the distribution of classifying observations for each class.


	\subsection{Regression predictive modeling} \label{sec:regression}
	In contrast to classification problems, such as classifying incoming email as spam or not spam, regression problems are about predicting continous quantaties. Regression algorithms can have either real-valued or discrete input variables. The model in eq. \ref{eq:example_ws} is an example of a regression problem since the goal is to predict a numerical value for wind speed. The problem could be translated into a classification problem by, for example stating that for given numerical intervals, the wind speed is categorized as being low, medium or high. This kind of conversion is known as discretization. But even if the conversion proves useful, it is beyond the scope of this project. 

%ta upp vilka performance measures är bäst?
	Performance of regression models can be measured by computing the mean squared error (MSE) of the model on the test dataset. 
\begin{equation}
	MSE_{test} = \frac{1}{n} \sum_{i}^{n}(y'_{test} - y_{test})_{i}^2
\end{equation}
where $y'_{test_i}$ are predictions on the test and $y_{test_i}$ are actual values. It's a measurement of how close each prediction was to its corresponding target value on average. Although other measurements can be used to evaluate regression models, such as R squared, \cite{BOOK:13} writes that the primary goal of any regression model evaluation should be to minimize MSE.


\section{Generalization} \label{sec:generalization}
	During the training process in supervised learning, a model is typically built based on its training data, and updated in order to reduce its training error. But the fundamental goal of machine learning is to generalize beyond observations in the training dataset since it's unlikely that the same exact observations are found again on unseen data \cite{ARTICLE:3}. Both training error, how well a performs on its training data, and generalization error, how well a model performs on unseen data, need to be considered in machine learning \cite{BOOK:1}.

	The terminology used to explain how well machine learning models learn and generalizes to new data is overfitting and underfitting. These are two central challenges in machine learning \cite{BOOK:1}. 
\begin{itemize}
	\item{Overfitting:} Random fluctuations and statisticical noise is learnt to the extent that it affects the model's ability to generalize. Instead of learning the data trend in the training data, the model "memorizes" it \cite{ARTICLE:4}.  
	\item{Underfitting:} A model that performs poorly on both its training data and on generalization. 
\end{itemize}
The goal then, is to select a model that is somewhere between underfitting and overfitting. Underfitting is typically remedied by choosing alternative models, but the most common problem in applied machine learning is how to avoid overfitting \cite{WEBSITE:8}. As a means to check whether or not a model suffers from overfitting, \cite{IP:7} suggests that when the test error of a model exceeds its training error, the model is overfitted to its training data. %however your training error is always lower than your test error. https://nlpers.blogspot.se/2015/09/overfitting.html, lägg till mer källor
According to Davide \cite{ARTICLE:4}  the mere awareness of the issue of overfitting along with two powerful tools: cross-validation and regularization, can be enough to overcome the problem. Feature selection is also brought up in this section as a means to overcome overfitting.

	\subsection{Cross-validation} 
	 An alternative, or perhaps complement, to the lock box approach as explained in \ref{sec:supervisedlearning} is Cross-validation. It is an approach where parts of the data are not necessarily used solely for testing, it can be used for both training and testing. Although the name might be confusing, the validation is typically done by the test data and a validation dataset is not necessarily used. One cross-validation technique is called $k$-fold cross-validation. In $k$-fold cross-validation, the data is randomly split into $k$ folds. The idea is to iterate the training and test process $k$ times so that every fold has been used once for testing, and ultimately average the performance over $k$ iterations . Using a value of $k = 10$ is a common choice in practice and in which case it is called 10-fold cross-validation \cite{ARTICLE:4}. Furthermore, using $k=10$ seems to be optimal when it comes to optimizing run-time for the test, limiting bias (underfitting) and variance (overfitting) \cite{BOOK:4}. While this technique can be used to limit overfitting, it also proves useful when dealing with small datasets since all of the data can be used for training \cite{ARTICLE:4}.  On the other hand, a different study shows that since $k$-fold uses its data both for training and evaluation, it isn't entirely unbiased and that it can create naïve models \cite{ARTICLE:25}.


%Cross-validation is an approach where the training data is used for both training and validation.  In cross-validation technique known as $k$-fold cross-validation, the data is randomly split into $k$ folds. The idea is to iterate the training and validation process $k$ times so that every fold has been used once for validation, and ultimately average the performance over $k$ iterations. This eliminates the need of leaving out a subset of the original dataset to be used as a validation dataset. This means that more data can be used for training instead. Using a value of $k = 10$ is a common choice in practice when using $k$-fold cross-validation, and in which case it is called 10-fold cross-validation \cite{ARTICLE:4}. Furthermore, using $k=10$ seems to be optimal when it comes to optimizing run-time for the test, limiting bias (underfitting) and variance (overfitting) \cite{BOOK:4}. By using this technique, the algorithm learns from each subset in the training set and limits the risk of overfitting to the training data \cite{ARTICLE:4}.


	There is a variant of this technique called stratified k-fold cross-validation. In stratified k-fold cross-validation the folds are created in such a way that each fold contains similar proportions of target features as the full dataset. For example, think of the classification problem of classifying email as spam or not spam. If this technique is applied to the email filtering problem as seen in \ref{sec:classification}, and the ratio of spam/not spam is 20\%/80\% in the original dataset, then the same proportion is attempted to be maintained in each of the $k$ folds. This technique tends to generate less bias and variance when compared to regular k-fold cross validation \cite{IP:2}. It can also be applied to regression problems but the results from Breiman and Spector \cite{ARTICLE:5} indicate that there is little improvement from using this technique for regression problems.

	\subsection{Regularization} \label{sec:regularization}
	Another method used to overcome overfitting is regularization. This technique discourages complexity and flexibility of models by regularizing its coefficients toward zero. It can be used by The magnitude of the regularization can be controlled by a hyperparameter $\lambda$. Hyperparameters are model parameters whose value are set before the training process. The higher value of $\lambda$, the higher impact regularization has on the model, but high values on $\lambda$ can result in underfitting and should therefore be controlled carefully \cite{WEBSITE:13}. Three different types of regularization methods:

\begin{itemize}
	\item{$L_1$ regularization (Lasso):} Adds a penalty equal to the sum of the absolute values of $n$ coefficients. This kind of regularization can nullify parameters and for that reason it can be seen as a way to limit the amount of features in the model (see \ref{sec:feature_selection}).
		\begin{equation}
			Error_{L_1} = Error + \lambda \sum_{i=1}^{n}|\beta_i| 
		\end{equation}
	\item{$L_2$ regularization (Ridge):} Adds a penalty equal to the sum of the square value of the coefficients. This exhibits a different behavior than $L_1$ regularization in that the coefficients are slowly reduced to zero.
		\begin{equation}
			Error_{L_2} = Error + \lambda \sum_{i=1}^{n}\beta_i^2 
		\end{equation}
	\item{$L_1/L_2$ regularization (Elastic-net):} A combination of $L_1$ and $L_2$ regularization. Here, a value $\alpha$ is set to determine the impact ratio of $L_1$ and $L_2$ regularization, where $0 \leq \alpha \leq 1$.
 		\begin{equation}
			Error_{L_1L_2} = Error + \lambda ( (1-\alpha)\sum_{i=1}^{n}|\beta_i| + \alpha  \sum_{i=1}^{n}\beta_i^2 ) 
		\end{equation}
\end{itemize}

	A comparison of these techniques was made by \cite{ARTICLE:21} in their performance of modelling insulin sensitivity. The results demonstrate a slight advantage using Lasso and Elastic-net in terms of performance. 
%This study examines different regularization approaches to investigate the solution stability of the method of fundamental solutions (MFS). We compare three regularization methods in conjunction with two different regularization parameters to find the optimal stable MFS scheme. Meanwhile, we have investigated the relationship among the condition number, the effective condition number, and the MFS solution accuracy. Numerical results show that the damped singular value decomposition under the parameter choice of the generalized cross-validation performs the best in terms of the MFS stability analysis. We also find that the condition number is a superior criterion to the effective condition number. (C) 2010 IMACS. Published by Elsevier B.V. All rights reserved.

	\subsection{Feature selection} \label{sec:feature_selection}
		Feature selection is a process where irrelevant and redundant features are removed. The curse of dimensionality, which is brought up in \ref{sec:background}, can be avoided by reducing the amount of features. According to \cite{ARTICLE:10}, a reduction in dimensionality can also lead to better generalization.  
	

%"Feature subset selection is the process of identifying and removing as many irrelevant and redundant features as possible. This reduces the dimensionality of the data and enables data mining algorithms to operate faster and more effectively. The fact that many features depend on one another often unduly influences the accurary of supervised ML classification models. this problem can be addressed by constructing new features from the basic feature set." \cite{ARTICLE:7}

	
\section{Supervised learning algorithms} \label{sec:supervised_algorithms}
	There are many algorithms that can be used to solve regression and classification problems, some of which can solve both. There is a famous theorem called the "no free lunch" theorem which contradicts an ideal case where a single machine learning is best at solving all possible machine learning problems \cite{ARTICLE:5}.  It is therefore interesting to study the effect of different supervised learning algorithms in this project, but to explain the functionality of them in detail is out of scope. Instead, the reader is encouraged to look up details on specific algorithms when needed. The algorithms are grouped by similarity in this section, which is referred to as algorithm families. The algorithm families are named the same way as in \cite{BOOK:6}. Each family listed in \ref{sec:delimitations} are covered. 

	\subsection{Decision tree based learning}
		Decision tree algorithms uses a decision tree datastructure to solve classification and regression problems. Furthermore, Non-leaf nodes represent conditions for a specific feature and leaves are values of the target feature. One of the main advantages of decision tree algorithms is that they are easy to understand \cite{ARTICLE:7}. Figure \ref{fig:decisiontree_example} depicts an example of a decision tree used to solve a classification problem with two features: sex and age. Starting at the root node, a decision is made once a leaf-node is reached. Ideally, the feature that best divides the dataset would be represented in the root of the tree, followed by the second best in the second level etc. However, constructing such optimal decision trees has been proved to be a NP-complete problem \cite{ARTICLE:11}. 

\begin{figure}[H] 
	\centering
	\includegraphics[width=0.8\textwidth]{media/decisiontree_example.png}
	\caption{Example of a decision tree which predicts if a passenger survives a car crash or not.}
	\label{fig:decisiontree_example}
\end{figure}

	%NÄR SKRIVA OM PARAMETRIC: But non-parametric approaches do suffer from a major disadvantage: since they do not reduce the problem of estimating f to a small number of parameters, a very large number of observations (far more than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for f \cite{BOOK:14}.
		Decision tree algorithms are allowed to capture nonlinear patterns in the training data, which makes them flexible, but also prone to overfitting \cite{BOOK:7}. Kotiantis \cite{ARTICLE:7} claims there are two common solutions to battle overfitting in decision tree induction algorithms:
		\begin{enumerate}
			\item Stop the training before it fits the training data perfectly
			\item Prune the decision tree
		\end{enumerate}
		The second method has shown to be more successful in practice than the first one \cite{BOOK:8}. Pruning is a process in which subtrees of the decision tree are replaced by leaves. In other words, the size of the tree is reduced. There are different pruning methods that can be used, but exploring such details are not in the scope of this project.

		 Among several decision tree induction algorithms, CART and C4.5 are the most commonly used \cite{BOOK:8}. CART can solve both regression and classification problems and it is available in Scikit-learn \cite{WEBSITE:16}. %CART can be used to solve multiclass classification problems \cite{ARTICLE:16}
		
	\subsection{Instance based learning} \label{sec:knn}
	Instance-based learning (IBL) is about storing the provided training data, and using it to predict/classify a new observation. In other words, when a new observation is received, an IBL algorithm retrieves related observations from memory and uses it to predict/classify the new observation. This behavior means that IBL algorithms process training data quickly while the classification/prediction process takes more time when compared to, for example, decision trees \cite{ARTICLE:8}. 

	An example of an IBL algorithm is $k$-nearest neighbor (kNN). Generally, observations can be considered to be points in an $n$-dimensional space and kNN is based on the principle that these observations in a dataset are generally close to other observations with similar properties.  For a new observation, the algorithm finds the $k$ closest neighbors and uses the properties of the $k$ neighbors to classify the observation. kNN can be used to solve both regression and multiclass classification problems in Scikit-learn \cite{WEBSITE:17, WEBSITE:18}.

	Wu et. al \cite{ARTICLE:9} argue that kNN is suitable for multiclass classification problems. Another study by L. Zhong et. al \cite{IP:3} showed success in predicting CT images from MRI data by using kNN regression. However, \cite{ARTICLE:9} mentions that the algorithm is sensitive to the choice of $k$: smaller values can mean it is sensitive to noise and larger values may include too many points from other classes. Overall, Okamoto and Yugami \cite{ARTICLE:12} showed that an optimal choice of $k$ grew linearly with an increase in the amount of training instances. This indicates that $k$ should be higher for higher amount of training data. However, scientific methods for choosing a specific optimal value of $k$  are lacking \cite{ARTICLE:7}.

	Wu et. al \cite{ARTICLE:9} talks about the issues of choosing a distance metric for kNN. They say that among several choices, it is desirable to have a distance measure in which a smaller value between two nodes implies a stronger connection. Scaling is another issue they talk about when it comes to distance metrics. If one feature $f_1$ varies from, say 1.5 to 1.8, and another $f_2$ from 10,000 to 1,000,000 then $f_2$ will have higher impact on the computation of distance. 
	
	%\subsection{Kernel methods based learning}
	
	\subsection{Bayesian learning}
		Bayesian algorithms are those that apply the Bayesian inference theorem. This can be used to calculate the probability of a hypothesis $H$ after seeing the data $D$.
		\begin{equation} \label{eq:bayes}
			p(H|D) = \frac{p(H)p(D|H)}{p(D)}
		\end{equation} 
	The basic notion of this kind of algorithms is that classification can be achieved if assumptions can be made from existing data. %Imagine an example of seeing a person whose sex you cannot decide, and you wish to address this person from afar. The person is standing in queue for the men's room, would you call out "excuse me ma'am" or "excuse me sir"? One may assume that this is a man since there is a higher probability of men being in line for the men's room than vice versa. 

	Naïve Bayes classifiers assume that the features in a dataset are independent from one another. Which means that each feature contribute independently to classify an observation, regardless of any correlations that might exist among the features, which is why it is called naïve. Despite this naïve assumption and its ease of use, Naïve Bayes classifiers have proved successful, even when strong dependencies among features are present \cite{ARTICLE:13, IP:4}. Naïve Bayes can be applied to regression problems through discretization, but it has limited success in comparison to classification problems when the independence assumption does not hold \cite{ARTICLE:14}. Naïve Bayes can be applied to multiclass classification problems \cite{ARTICLE:17}. 

	The Gaussian Naiïve Bayes classifier assumes that the features have a gaussian (normal) data distribution.
	 
	\subsection{Regression based learning} \label{sec:reg_based_learning}
		As stated in \ref{sec:delimitations}, only linear algorithms of Regression based learning agorithms are covered. A linear algorithm is one where its model is specified as a linear combination of input features.  

		Although the names might be confusing, regression based learning is not the same as regression predictive modelling, whose type of problems are referred to as "regression problems" throughout this project. Regression based analysis refers to regression analysis: a number of statistical methods for estimating relationships among variables. This is a well-known tool in statistics and it is also used in machine learning. 

		There are three components in a regression model: scalars $\beta_i$, independent variables $X_i$ and dependent variables $Y$. In regression based learning, $X_i$ represent input features, $Y$ is the target feature and $\beta_i$ are parameters which are tuned during the training process. The example shown in \ref{eq:example_ws} is an example of a trained regression based learning algorithm called Ordinary Least Squares (OLS). OLS is used in regression predictive modelling and a different regression based learning algorithm called Logistic regression (LR) can be used for classification problems. 

		Some of the assumptions listed by \cite{BOOK:6} that most regression based learning algorithms make of the training data:
	\begin{itemize}
		\item{Linear behavior:} If a target feature and an input feature have a linear relationship, it would look like a straight line when plotted against oneanother. Any non-linear dependencies between the target feature and other features are assumed to not be considered input features.
		\item{Normal distribution:} Assumes the data of the target feature is normally distributed.
		\item{Outliers:} Are expected to be handled.
		\item{Data accuracy:} Values of the target feature that are consider for classification/prediction are assumed to be deleted. For example, error states that are not to be classified.
	\end{itemize} %skriv mer om assumptions?

		In addition to applying cross-validation, overfitting in regression based learning algorithms can be thwarted by applying the regularization techniques as seen in \ref{sec:regularization}. Feng et. al \cite{ARTICLE:15} chose $\lambda$, which is one of the regularization hyperparameters,  based on which value of $\lambda$ maximised their cross-validation scores. They did so by performing a grid-search (see \ref{sec:hyperparameters}) on $\lambda$. %ex: lambda = e^(x/10), x = -100, -99,...9, 10).http://eds.b.ebscohost.com.proxy.lib.ltu.se/eds/pdfviewer/pdfviewer?vid=6&sid=e3b5f9cf-09c2-4142-81c9-bbfa4b659571@sessionmgr4008

	\subsection{Deep learning}
			Deep learning algorithms are those that aim to learn an artificial neural network (ANN) \cite{BOOK:6}. ANNs take inspiration from biological neural networks found in the human brain. An ANN is a directed weighted graph where each node represents an artificial neuron. The graph is organized from left to right by having three type of nodes:
	\begin{itemize}
		\item Input nodes
		\item Hidden layer nodes
		\item Output nodes
	\end{itemize}
			The input nodes are the different input features and the output nodes are possible outcomes. Without going into too much detail, the hidden layers are there to transform the input to an output. For example, imagine a neural network model trained to recognize triangles. The input nodes represent all of the different pixels. The first hidden layer of that model distinguishes edges in the picture, the second layer recognizes when these edges form complex shapes, and so on. Figure \ref{fig:neuralnetwork_example} depicts an example of a deep learning algorithm model with the same kind of problem as seen in \ref{fig:decisiontree_example}.

\begin{figure}[H] 
	\centering
	\includegraphics[width=0.8\textwidth]{media/neuralnetwork_ex.png}
	\caption{Example of a neural network with two hidden layers. The goal is to predict if a passenger survives a car crash or not.}
	\label{fig:neuralnetwork_example}
\end{figure}

			The number of hidden layers $h$ and the number of nodes per hidden layer $N_h$ can be configured as hyperparameters in the different deep learning methods. So the question is, what is the optimal value of $h$ and $N_h$? Heaton \cite{BOOK:10} writes that for most practical problems, having $h=1$ is enough for most practical problems and that there is no theoretical reason to use more than two hidden layers. As for $N_h$, Heaton writes that a low value of $N_h$ can lead to underfitting and the opposite may introduce overfitting and a significant increase in training time. The author encourages practitioners to try different values for $N_h$ but provides several rule-of-thumb methods for simplicity, one of which is the following: "The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer". %"The Presence of irrelevant features can make neural network training very inefficient, even impractical" \cite{ARTICLE:7}

			All that's been covered so far of ANNs is the structure, but how are decisions made? Choices are made in the ANN by having each non-input node $n_i$ process information from the previous node whose edge to $n_i$ have the highest value among all nodes connected to $n_i$. In other words, which of the connected nodes to $n_i$ to choose depends on the weights of the edges that connects them. How the weights are set correlates to the learning process, which is what distinguishes the different types of deep learning algorithms.

			Backpropagation is a deep learning algorithm that can be used to solve supervised learning problems. Broadly speaking, backpropagation updates the weights of the edges in the ANN based on a loss function. Whenever a "bad" classification/prediction is made (based on the loss function), the information is propagated backwards in the network from that classification/prediction and weights are adjusted accordingly.

	\subsection{Ensemble learning}
		Ensemble learning is based on the principle that combining the results of several classifiers/predictors into a collective score can produce better results than individual scores. Two independent studies indicate that Random forest, an ensemble learning algorithm, show high success when compared to non-ensemble algorithms\cite{ARTICLE:16, IP:5}. %Chowdhury et. al \cite{ARTICLE:16} conducted a study to see if ensemble methods were superior to CART, kNN, support vector machine and neural network in classifying physical activities. One of the ensemble methods used in the study was Random forest, which showed a constistent high score. Random forest performed second best in an empirical comparison of ten supervised learning algorithms \cite{IP:5}.
		Random forest averages the results from several decision tree classifiers/predictors to achieve a result. Random forest can be applied to classification and regression problems \cite{WEBSITE:17}.

\section{Algorithm selection}
	Selecting the optimal algorithms among all of the algorithms in \ref{sec:supervised_algorithms} may prove cumbersome. One approach could be to study the benefits and advantages of each algorithm or algorithm family in detail and decide beforehand which one to choose. Microsoft Azure and Scikit Learn provide intuitive guides of which specific algorithm to choose depending on size of dataset and which the type of problem \cite{WEBSITE:20, WEBSITE:21}. However similar algorithm-choosing guides seems to be few in academic papers, perhaps because there are many different situations, hyperparameters, data sizes etc .
	%The experimental results shows that there's a major distinction within the accuracy of a same algorithm once applied on three completely different datasets. (3 classification problems, one 617 observations 76 attributes, 569 observations 76 attributes, ~600 observations 14 attributes) \cite{ARTICLE:22}.

	Another approach to selecting an optimal algorithm is experimental.

	%tip 1 \cite{ARTICLE:4}
	%tip 4, choose simple algorithm \cite{ARTICLE:4}
\section{Optimizing hyperparameters} \label{sec:hyperparameters}
	Generally speaking, finding optimal hyperparameter values for various algorithms can be regarded as an experimental process. One brute-force approach, which according to \cite{BOOK:9} is the best way to verify optimality of hyperparameters, is to use a technique known as grid-search. The technique systematically tests hyperparameters incrementally in a given interval. Although the technique may prove useful, it can be computationally intense.

	Another way to test parameters is to use

	%"Note that if we optimized the hyperparameters based on a validation score the validation score is biased and not a good estimate of the generalization any longer."  http://scikit-learn.org/stable/modules/learning_curve.html

	%tip 6 \cite{ARTICLE:4}

	%The best way to verify hyperparameters for an algorithm applied to specific data is to test them all by cross-validation, and to pick the best combination. This simple approach, called grid-search, offers indisputable advantages by allowing you to sample the range of possible values to input into the algorithm systematically and to spot when the thengeral minimum happens. On the other hand, grid-search has two disadvantages:
	%computationally intense
	% systematic and intensive tests enhance possibility of incurring error because some good but fake validation results can be caused by noise present in the dataset. \cite{BOOK:9}





%\begin{comment}
	
\section{Data preparation}
	%tip 1 \cite{ARTICLE:4}
	
	%tip 5 \cite{ARTICLE:4}
	\subsection{Imbalanced data} \label{sec:imbalancedtheory}
	A number of oversampling techniques that can be used to solve multiclass classification problems are suggested by \ref{IP:6}. However the suggested techniques are complex and can be integrated manually to comply with Scikit-learn. Imbalanced-learn is a Scikit-learn compatible Python module that can be used to perform oversampling \cite{WEBSITE:22}. Three techniques are supported for oversampling: Smote, Adasyn and Randomoversampler. The techniques can be used to solve multiclass classification problems in Imbalanced-learn, despite not being suggested by \ref{IP:6} as proper techniques to solve such problems. 

Smote and Adasyn creates new synthetic points in the dataset while Randomoversampler creates duplicates. Synthetic points are points that are created from existing points but with small random adjustments. The disadvantages of using duplicate observations instead of synthetic points is that it introduces overfitting \cite{BOOK:12}. However, both Adasyn and Smote can be configured in a number of ways on how the synthetic points are built, and are therefore not as easy to use as Randomoversampler \cite{WEBSITE:23}. 
% tip 5 \cite{ARTICLE:4} https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5721660/


% binarization with boosting and oversampling (BBO) oversampling technique proved useful for multiclass classification \cite{ARTICLE:23}

	%https://machinelearningmastery.com/how-to-prepare-data-for-machine-learning/

	%method to filter out noise, outliers http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6033571

%\end{comment}
