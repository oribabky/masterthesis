%OBS. Vid sex eller flera författare anges endast den första följt av et al. (vilket betyder "och de andra" på latin).

@BOOK{BOOK:1,
    	title="Deep Learning",
    	author="I. Goodfellow and Y. Bengio and A. Courville",
    	publisher="MIT Press",
    	year="2016"
}
@BOOK{BOOK:2,
    	title="Machine Learning",
    	author="T. Mitchell",
    	publisher="McGraw Hill",
    	year="1997"
}

@BOOK{BOOK:3,
Author = {M. Mehryar and R. Afshin and T. Ameet},
ISBN = {978-0-262-01825-8},
Publisher = {MIT Press, Cambridge, MA},
Series = {Adaptive Computation and Machine Learning.},
Title = {Foundations of machine learning.},
URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=msn&AN=MR3057769&lang=sv&site=eds-live&scope=site},
Year = {2012},
}

@BOOK{BOOK:4,
Author = {Breiman, Leo and Friedman, Jerome H. and Olshen, Richard A. and Stone, Charles J.},
ISBN = {0-534-98053-8},
Publisher = {Wadsworth Advanced Books and Software, Belmont, CA},
Series = {Wadsworth Statistics/Probability Series.},
Title = {Classification and regression trees.},
URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=msn&AN=MR726392&lang=sv&site=eds-live&scope=site},
Year = {1984},
}

@BOOK{BOOK:5,
 author = {Dunning, Ted and Friedman, Ellen},
 title = {Practical Machine Learning: Innovations in Recommendation},
 year = {2014},
 isbn = {1491915382, 9781491915387},
 edition = {1st},
 publisher = {O'Reilly Media, Inc.},
} 

@BOOK{BOOK:6,
  title={Practical Machine Learning},
  author={Gollapudi, S.},
  isbn={9781784399689},
  series={Community experience distilled},
  url={https://books.google.se/books?id=3ywhjwEACAAJ},
  year={2016},
  publisher={Packt Publishing}
}

@book{BOOK:7,
  title={Machine Learning: ECML 2003: 14th European Conference on Machine Learning, Cavtat-Dubrovnik, Croatia, September 22-26, 2003, Proceedings},
  author={Lavra{\v{c}}, N.},
  number={v. 14},
  isbn={9783540201212},
  lccn={03059180},
  series={Lecture Notes in Artificial Intelligence},
  url={https://books.google.se/books?id=C\_E4lDNIR1QC},
  year={2003},
  publisher={Springer}
}

@book{BOOK:8,
 author = {Mitchell, Thomas M.},
 title = {Machine Learning},
 year = {1997},
 isbn = {0070428077, 9780070428072},
 edition = {1},
 publisher = {McGraw-Hill, Inc.},
 address = {New York, NY, USA},
} 

@book{BOOK:9,
Abstract = {Your no-nonsense guide to making sense of machine learning Machine learning can be a mind-boggling concept for the masses, but those who are in the trenches of computer programming know just how invaluable it is. Without machine learning, fraud detection, web search results, real-time ads on web pages, credit scoring, automation, and email spam filtering wouldn't be possible, and this is only showcasing just a few of its capabilities. Written by two data science experts, Machine Learning For Dummies offers a much-needed entry point for anyone looking to use machine learning to accomplish practical tasks. Covering the entry-level topics needed to get you familiar with the basic concepts of machine learning, this guide quickly helps you make sense of the programming languages and tools you need to turn machine learning-based tasks into a reality. Whether you're maddened by the math behind machine learning, apprehensive about AI, perplexed by preprocessing data—or anything in between—thi},
Author = {Mueller, John and Massaron, Luca},
ISBN = {9781119245513},
Publisher = {For Dummies},
Series = {For Dummies},
Title = {Machine Learning For Dummies.},
URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=nlebk&AN=1237397&lang=sv&site=eds-live&scope=site},
Year = {2016},
}

@book{BOOK:10,
  title={Introduction to Neural Networks with Java},
  author={Heaton, J.},
  isbn={9781604390087},
  url={https://books.google.se/books?id=Swlcw7M4uD8C},
  year={2008},
  publisher={Heaton Research}
}




@ARTICLE{ARTICLE:1,
	author = "A.K. Arvidsson",
    	title = "The Winter Model – A new way to calculate socio-economic costs depending on winter maintenance strategy",
    	journal = "Cold Regions Science and Technology
Volume 136, April 2017, Pages 30-36",
    	year = "2017",
}

@ARTICLE{ARTICLE:2,
	author = "A.K. Arvidsson",
    	title = "The Winter Model – A new way to calculate socio-economic costs depending on winter maintenance strategy",
    	journal = "Cold Regions Science and Technology
Volume 136, April 2017, Pages 30-36",
    	year = "2017",
}

@article{ARTICLE:3,
	Author = {Domingos, Pedro},
	ISSN = {00010782},
	Journal = {Communications of the ACM},
	Keywords = {MACHINE theory, COMPUTER science, ALGORITHMS, ANALYSIS of variance, DATA mining, COMPUTER programming, BAYESIAN analysis, 		DECISION trees, SCALABILITY (Systems engineering), MACHINE learning, COMPUTATIONAL learning theory, GENERALIZATION},
	Number = {10},
	Pages = {78 - 87},
	Title = {A Few Useful Things to Know About Machine Learning.},
	Volume = {55},
	URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=82151052&lang=sv&site=eds-		live&scope=site},
	Year = {2012},
}

@article{ARTICLE:4,
	Abstract = {Machine learning has become a pivotal tool for many projects in computational biology, bioinformatics, and health informatics. Nevertheless, beginners and biomedical researchers often do not have enough experience to run a data mining project effectively, and therefore can follow incorrect practices, that may lead to common mistakes or over-optimistic results. With this review, we present ten quick tips to take advantage of machine learning in any computational biology context, by avoiding some common errors that we observed hundreds of times in multiple bioinformatics projects. We believe our ten suggestions can strongly help any machine learning practitioner to carry on a successful project in computational biology and related sciences. [ABSTRACT FROM AUTHOR]},
Author = {Chicco, Davide},
ISSN = {17560381},
Journal = {BioData Mining},
Keywords = {MACHINE learning, BIOINFORMATICS, DATA mining, COMPUTATIONAL biology, COMPUTATIONAL neuroscience, Bioinformatics, Biomedical informatics, Computational biology, Computational intelligence, Data mining, Health informatics, Machine learning, Tips},
Pages = {1 - 17},
Title = {Ten quick tips for machine learning in computational biology.},
Volume = {10},
URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=aph&AN=126676440&lang=sv&site=eds-live&scope=site},
Year = {2017},
}

@ARTICLE{ARTICLE:5,
ISSN = {03067734},
Journal = {INTERNATIONAL STATISTICAL REVIEW},
Keywords = {REGRESSION, VARIABLE SELECTION, CROSS-VALIDATION, BOOTSTRAP, PREDICTION ERROR, SUBSET SELECTION, Statistics & Probability, Mathematics},
Number = {3},
Pages = {291 - 319},
Title = {SUBMODEL SELECTION AND EVALUATION IN REGRESSION - THE X-RANDOM CASE.},
Volume = {60},
URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=edswsc&AN=A1992KC62300004&lang=sv&site=eds-live&scope=site},
Year = {n.d.},
}

@ARTICLE{ARTICLE:6,
Author = {Wolpert, D.H. ( 1 ) and Macready, W.G. ( 2 )},
ISSN = {1089778X},
Journal = {IEEE Transactions on Evolutionary Computation},
Keywords = {Evolutionary algorithms, Information theory, Optimization},
Number = {1},
Pages = {67-82},
Title = {No free lunch theorems for optimization.},
Volume = {1},
URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=edselc&AN=edselc.2-52.0-0031118203&lang=sv&site=eds-live&scope=site},
Year = {1997},
}

@article{ARTICLE:7,
Author = {Kotsiantis, S.B.},
ISSN = {03505596},
Journal = {Informatica (Ljubljana)},
Keywords = {Classifiers, Data mining techniques, Intelligent data analysis, Learning algorithms},
Number = {3},
Pages = {249-268},
Title = {Supervised machine learning: A review of classification techniques.},
Volume = {31},
URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=edselc&AN=edselc.2-52.0-36749047332&lang=sv&site=eds-live&scope=site},
Year = {2007},
}

@article{ARTICLE:8,
title = "An efficient algorithm for optimal pruning of decision trees",
journal = "Artificial Intelligence",
volume = "83",
number = "2",
pages = "347 - 362",
year = "1996",
issn = "0004-3702",
doi = "https://doi.org/10.1016/0004-3702(95)00060-7",
url = "http://www.sciencedirect.com/science/article/pii/0004370295000607",
author = "Hussein Almuallim"
}

@article{ARTICLE:9,
Author = {Wu, X. ( 1 ) and Kumar, V. ( 2 ) and Steinbach, M. ( 2 ) and Ross, Q.J. ( 3 ) and Ghosh, J. ( 4 ) and Yang, Q. ( 5 ) and Motoda, H. ( 6 ) and McLachlan, G.J. ( 7 ) and Ng, A. ( 8 ) and Liu, B. ( 9 ) and Yu, P.S. ( 10 ) and Zhou, Z.-H. ( 11 ) and Hand, D.J. ( 12 ) and Steinberg, D. ( 13 )},
ISSN = {02191377},
Journal = {Knowledge and Information Systems},
Number = {1},
Pages = {1-37},
Title = {Top 10 algorithms in data mining.},
Volume = {14},
URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=edselc&AN=edselc.2-52.0-37549018049&lang=sv&site=eds-live&scope=site},
Year = {2008},
}

@article{ARTICLE:10,
Author = {Bermingham, M.L. ( 1 ) and Spiliopoulou, A. ( 1 ) and Hayward, C. ( 1 ) and Wright, A.F. ( 1 ) and Navarro, P. ( 1 ) and Haley, C.S. ( 1,2 ) and Pong-Wong, R. ( 2 ) and Rudan, I. ( 3 ) and Campbell, H. ( 3 ) and Wilson, J.F. ( 3 ) and Agakov, F. ( 4 )},
ISSN = {20452322},
Journal = {Scientific Reports},
Title = {Application of high-dimensional feature selection: Evaluation for genomic prediction in man.},
Volume = {5},
URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=edselc&AN=edselc.2-52.0-84930221871&lang=sv&site=eds-live&scope=site},
Year = {2015},
}

@article{ARTICLE:11,
Author = {Hyafil, L. ( 1 ) and Rivest, R.L. ( 2 )},
ISSN = {00200190},
Journal = {Information Processing Letters},
Keywords = {Binary decision trees, computational complexity, NP-complete},
Number = {1},
Pages = {15-17},
Title = {Constructing optimal binary decision trees is NP-complete.},
Volume = {5},
URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=edselc&AN=edselc.2-52.0-0001815269&lang=sv&site=eds-live&scope=site},
Year = {1976},
}

@article{ARTICLE:12,
title = "Effects of domain characteristics on instance-based learning algorithms",
journal = "Theoretical Computer Science",
volume = "298",
number = "1",
pages = "207 - 233",
year = "2003",
note = "Selected Papers in honour of Setsuo Arikawa",
issn = "0304-3975",
doi = "https://doi.org/10.1016/S0304-3975(02)00424-3",
url = "http://www.sciencedirect.com/science/article/pii/S0304397502004243",
author = "Seishi Okamoto and Nobuhiro Yugami",
keywords = "Instance-based learning, -Nearest neighbor classifier, Average-case analysis, Expected accuracy, Optimal value of "
}

@Article{ARTICLE:13,
author="Domingos, Pedro
and Pazzani, Michael",
title="On the Optimality of the Simple Bayesian Classifier under Zero-One Loss",
journal="Machine Learning",
year="1997",
month="Nov",
day="01",
volume="29",
number="2",
pages="103--130",
abstract="The simple Bayesian classifier is known to be optimal when attributes are independent given the class, but the question of whether other sufficient conditions for its optimality exist has so far not been explored. Empirical results showing that it performs surprisingly well in many domains containing clear attribute dependences suggest that the answer to this question may be positive. This article shows that, although the Bayesian classifier's probability estimates are only optimal under quadratic loss if the independence assumption holds, the classifier itself can be optimal under zero-one loss (misclassification rate) even when this assumption is violated by a wide margin. The region of quadratic-loss optimality of the Bayesian classifier is in fact a second-order infinitesimal fraction of the region of zero-one optimality. This implies that the Bayesian classifier has a much greater range of applicability than previously thought. For example, in this article it is shown to be optimal for learning conjunctions and disjunctions, even though they violate the independence assumption. Further, studies in artificial domains show that it will often outperform more powerful classifiers for common training set sizes and numbers of attributes, even if its bias is a priori much less appropriate to the domain. This article's results also imply that detecting attribute dependence is not necessarily the best way to extend the Bayesian classifier, and this is also verified empirically.",
issn="1573-0565",
doi="10.1023/A:1007413511361",
url="https://doi.org/10.1023/A:1007413511361"
}

@Article{ARTICLE:14,
author="Frank, Eibe
and Trigg, Leonard
and Holmes, Geoffrey
and Witten, Ian H.",
title="Technical Note: Naive Bayes for Regression",
journal="Machine Learning",
year="2000",
month="Oct",
day="01",
volume="41",
number="1",
pages="5--25",
abstract="Despite its simplicity, the naive Bayes learning scheme performs well on most classification tasks, and is often significantly more accurate than more sophisticated methods. Although the probability estimates that it produces can be inaccurate, it often assigns maximum probability to the correct class. This suggests that its good performance might be restricted to situations where the output is categorical. It is therefore interesting to see how it performs in domains where the predicted value is numeric, because in this case, predictions are more sensitive to inaccurate probability estimates.",
issn="1573-0565",
doi="10.1023/A:1007670802811",
url="https://doi.org/10.1023/A:1007670802811"
}

@article{ARTICLE:15,
Abstract = {Graphical models are frequently used to explore networks, such as genetic networks, among a set of variables. This is usually carried out via exploring the sparsity of the precision matrix of the variables under consideration. Penalized likelihood methods are often used in such explorations. Yet, positive-definiteness constraints of precision matrices make the optimization problem challenging. We introduce nonconcave penalties and the adaptive LASSO penalty to attenuate the bias problem in the network estimation. Through the local linear approximation to the nonconcave penalty functions, the problem of precision matrix estimation is recast as a sequence of penalized likelihood problems with a weighted L(1) penalty and solved using the efficient algorithm of Friedman et al. [Biostatistics 9 (2008) 432-441]. Our estimation schemes are applied to two real datasets. Simulation experiments and asymptotic theory are used to justify our proposed methods.},
Author = {Feng, Yang and Fan, Jianqing and Feng, Yang and Wu, Yichao},
ISSN = {19326157},
Journal = {ANNALS OF APPLIED STATISTICS},
Keywords = {Adaptive LASSO, covariance selection, Gaussian concentration graphical model, genetic network, LASSO, precision matrix, SCAD, Statistics & Probability, Mathematics},
Number = {2},
Pages = {521 - 541},
Title = {NETWORK EXPLORATION VIA THE ADAPTIVE LASSO AND SCAD PENALTIES.},
Volume = {3},
URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=edswsc&AN=000271979600002&lang=sv&site=eds-live&scope=site},
Year = {n.d.},
}

@article{ARTICLE:16,
Author = {CHOWDHURY, ALOK KUMAR and TJONDRONEGORO, DIAN and CHANDRAN, VINOD and TROST, STEWART G.},
ISSN = {01959131},
Journal = {Medicine and Science in Sports and Exercise},
Keywords = {*CLASSIFICATION, *WRIST, *MOTION capture (Medicine), *PHYSICAL activity, ALGORITHMS, DECISION trees, ACCELEROMETRY, BAGGING, BOOSTED DECISION TREES, MACHINE LEARNING, MOTION SENSORS, PATTERN RECOGNITION, RANDOM FOREST},
Number = {9},
Pages = {1965 - 1973},
Title = {Ensemble Methods for Classification of Physical Activities from Wrist Accelerometry.},
Volume = {49},
URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=s3h&AN=124667245&lang=sv&site=eds-live&scope=site},
Year = {2017},
}

@article{ARTICLE:17,
  added-at = {2013-01-10T07:09:13.000+0100},
  author = {Aly, M.},
  biburl = {https://www.bibsonomy.org/bibtex/29184db6dce2d01c92a664db199a14118/lychen1109},
  interhash = {5db3682afcaaaacba2c00479f92c4c6c},
  intrahash = {9184db6dce2d01c92a664db199a14118},
  journal = {Neural networks},
  keywords = {classification multiclass},
  pages = {1-9},
  timestamp = {2013-01-10T07:09:13.000+0100},
  title = {Survey on multiclass classification methods},
  year = 2005
}

@INPROCEEDINGS{IP:1, 
	author={S. Bhatia and J. Malhotra}, 	
	booktitle={2014 International Conference on Advances in Engineering Technology Research (ICAETR - 2014)}, 	
	title={A survey on impact of lines of code on software complexity}, 
	year={2014}, 
	volume={}, 
	number={}, 
	pages={1-4}, 
	keywords={computational complexity;software metrics;source code (software);SLOC;lines of code;problem complexity;programming productivity;project size;software complexity;software industry;software system;source code;Biological system modeling;Complexity theory;Estimation;Size 			measurement;Software;Software measurement;Complexity;Effort;Lines of code component}, 	
	doi={10.1109/ICAETR.2014.7012875}, 
	ISSN={2347-9337}, 
	month={Aug},
}

@INPROCEEDINGS{IP:2,
    author = {Ron Kohavi},
    title = {A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection},
    booktitle = {},
    year = {1995},
    pages = {1137--1143},
    publisher = {Morgan Kaufmann}
}

@INPROCEEDINGS{IP:3, 
author={L. Zhong and L. Lin and Z. Lu and Y. Wu and Z. Lu and M. Huang and W. Yang and Q. Feng}, 
booktitle={2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)}, 
title={Predict CT image from MRI data using KNN-regression with learned local descriptors}, 
year={2016}, 
volume={}, 
number={}, 
pages={743-746}, 
keywords={biomedical MRI;computerised tomography;learning (artificial intelligence);medical image processing;radiation therapy;regression analysis;CT image prediction;KNN regression;MR based radiation therapy;MRI data;PET-MR hybrid imaging systems;attenuation correction;dose planning;learned local descriptors;low rank approximation;manifold regularization;supervised descriptor learning;Attenuation;Computed tomography;Learning systems;Magnetic resonance imaging;Manifolds;Training;CT prediction;PET attenuation correction;local descriptor learning;low-rank approximation}, 
doi={10.1109/ISBI.2016.7493373}, 
ISSN={}, 
month={April},}

@INPROCEEDINGS{IP:4,
    author = {Harry Zhang},
    title = {The Optimality of Naïve Bayes},
    booktitle = {In FLAIRS2004 conference},
    year = {2004}
}

@inproceedings{IP:5,
 author = {Caruana, Rich and Niculescu-Mizil, Alexandru},
 title = {An Empirical Comparison of Supervised Learning Algorithms},
 booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
 series = {ICML '06},
 year = {2006},
 isbn = {1-59593-383-2},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {161--168},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1143844.1143865},
 doi = {10.1145/1143844.1143865},
 acmid = {1143865},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@ELECTRONIC{WEBSITE:1,
	url = "https://machinelearningmastery.com/classification-versus-regression-in-machine-learning/",
	author = "Jason Brownlee",
	title = "Difference Between Classification and Regression in Machine Learning",
	year = "2017",
	urldate = "2018-01-29", 
}

@ELECTRONIC{WEBSITE:2,
	url = "https://www.trafikverket.se/resa-och-trafik/underhall-av-vag-och-jarnvag/Sa-skoter-vi-vagar/Vintervaghallning/",
	author = "Trafikverket",
	title = "Vinterväghållning",
	year = "2017",
	urldate = "2018-02-07", 
}

@ELECTRONIC{WEBSITE:3,
	url = "https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/",
	author = "J. Brownlee",
	title = "Supervised and Unsupervised Machine Learning Algorithms",
	year = "2016",
	urldate = "2018-02-28", 
}

@ELECTRONIC{WEBSITE:4,
	url = "https://techcrunch.com/2017/05/31/google-says-its-machine-learning-tech-now-blocks-99-9-of-gmail-spam-and-phishing-messages/",
	author = "F. Lardinois",
	title = "Google says its machine learning tech now blocks 99.9\% of Gmail spam and phishing messages",
	year = "2017",
	urldate = "2018-03-19", 
}

@ELECTRONIC{WEBSITE:5, 
	url = "https://archive.ics.uci.edu/ml/datasets/iris",
	author = "R.A. Fisher",
	title = "Iris Data Set",
	urldate = "2018-03-19"
}

@ELECTRONIC{WEBSITE:6, 
	url = "https://machinelearningmastery.com/difference-test-validation-datasets/",
	author = "J. Brownlee",
	title = "What is the Difference Between Test and Validation Datasets?",
	urldate = "2018-03-20"
}

@ELECTRONIC{WEBSITE:7, 
	url = "https://www.quora.com/What-are-hyperparameters-in-machine-learning",
	author = "X. Amatriain",
	title = "What are hyperparameters in machine learning?",
	urldate = "2018-03-20"
}

@ELECTRONIC{WEBSITE:8, 
	url = "https://machinelearningmastery.com/classification-versus-regression-in-machine-learning/",
	author = "J. Brownlee",
	title = "Difference Between Classification and Regression in Machine Learning",
	urldate = "2018-03-22"
}

@ELECTRONIC{WEBSITE:9, 
	url = "https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/",
	author = "J. Brownlee",
	title = "Overfitting and Underfitting With Machine Learning Algorithms",
	urldate = "2018-03-22"
}

@ELECTRONIC{WEBSITE:10, 
	url = "http://scottclowe.com/2016-03-19-stratified-regression-partitions/",
	author = "S.C. Lowe",
	title = "Stratified Validation Splits for Regression Problems",
	urldate = "2018-03-29"
}

@ELECTRONIC{WEBSITE:11, 
	url = "https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/",
	author = "Jason Brownlee",
	title = "Overfitting and Underfitting With Machine Learning Algorithms",
	urldate = "2018-03-29"
}

@ELECTRONIC{WEBSITE:12, 
	url = "https://www.r-bloggers.com/machine-learning-explained-regularization/",
	author = "EnhanceDataScience",
	title = "Machine Learning Explained: Regularization",
	urldate = "2018-03-28"
}

@ELECTRONIC{WEBSITE:13, 
	url = "https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a",
	author = "Prashant Gupta",
	title = "Regularization in Machine Learning",
	urldate = "2018-04-03"
}

@ELECTRONIC{WEBSITE:14, 
	url = "https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/",
	author = "Jason Brownlee",
	title = "A tour of Machine Learning Algorithms",
	urldate = "2018-04-05"
}

@ELECTRONIC{WEBSITE:15, 
	url = "http://scikit-learn.org/stable/supervised_learning.html",
	author = "scikit-learn developers",
	title = "Supervised Learning",
	urldate = "2018-04-05"
}

@ELECTRONIC{WEBSITE:16, 
	url = "http://scikit-learn.org/stable/modules/tree.html",
	author = "scikit-learn developers",
	title = "Decision trees",
	urldate = "2018-04-10"
}

@ELECTRONIC{WEBSITE:17, 
	url = "http://scikit-learn.org/stable/modules/classes.html",
	author = "scikit-learn developers",
	title = "API Reference",
	urldate = "2018-04-10"
}

@ELECTRONIC{WEBSITE:18, 
	url = "http://scikit-learn.org/stable/modules/multiclass.html",
	author = "scikit-learn developers",
	title = "Multiclass and multilabel algorithms",
	urldate = "2018-04-10"
}





@MISC{MISC:1,
    author = {Mohamed Aly},
    title = {Survey on Multiclass Classification Methods},
    year = {2005}
}




@ELECTRONIC{IMAGE:1,
	url = "https://commons.wikimedia.org/wiki/File:Rwis_station_Myggsjon_01.JPG",
	author = "Pelpet",
	title = "Rwis Station at sensor site Myggsjön",
	year = "2010",
	urldate = "2018-02-06", 
}

@MISC{REPORT:1,
  	title = "Trafikverkets årsredovisning 2015",
	author = "Trafikverket",
  	year = "2016",
}



