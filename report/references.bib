%OBS. Vid sex eller flera författare anges endast den första följt av et al. (vilket betyder "och de andra" på latin).

@BOOK{BOOK:1,
    	title="Deep Learning",
    	author="I. Goodfellow and Y. Bengio and A. Courville",
    	publisher="MIT Press",
    	year="2016"
}
@BOOK{BOOK:2,
    	title="Machine Learning",
    	author="T. Mitchell",
    	publisher="McGraw Hill",
    	year="1997"
}

@BOOK{BOOK:3,
Author = {M. Mehryar and R. Afshin and T. Ameet},
ISBN = {978-0-262-01825-8},
Publisher = {MIT Press, Cambridge, MA},
Series = {Adaptive Computation and Machine Learning.},
Title = {Foundations of machine learning.},
URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=msn&AN=MR3057769&lang=sv&site=eds-live&scope=site},
Year = {2012},
}

@BOOK{BOOK:4,
Author = {Breiman, Leo and Friedman, Jerome H. and Olshen, Richard A. and Stone, Charles J.},
ISBN = {0-534-98053-8},
Publisher = {Wadsworth Advanced Books and Software, Belmont, CA},
Series = {Wadsworth Statistics/Probability Series.},
Title = {Classification and regression trees.},
URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=msn&AN=MR726392&lang=sv&site=eds-live&scope=site},
Year = {1984},
}

@BOOK{BOOK:5,
 author = {Dunning, Ted and Friedman, Ellen},
 title = {Practical Machine Learning: Innovations in Recommendation},
 year = {2014},
 isbn = {1491915382, 9781491915387},
 edition = {1st},
 publisher = {O'Reilly Media, Inc.},
} 

@BOOK{BOOK:6,
  title={Practical Machine Learning},
  author={Gollapudi, S.},
  isbn={9781784399689},
  series={Community experience distilled},
  url={https://books.google.se/books?id=3ywhjwEACAAJ},
  year={2016},
  publisher={Packt Publishing}
}

@book{BOOK:7,
  title={Machine Learning: ECML 2003: 14th European Conference on Machine Learning, Cavtat-Dubrovnik, Croatia, September 22-26, 2003, Proceedings},
  author={Lavra{\v{c}}, N.},
  number={v. 14},
  isbn={9783540201212},
  lccn={03059180},
  series={Lecture Notes in Artificial Intelligence},
  url={https://books.google.se/books?id=C\_E4lDNIR1QC},
  year={2003},
  publisher={Springer}
}

@book{BOOK:8,
 author = {Mitchell, Thomas M.},
 title = {Machine Learning},
 year = {1997},
 isbn = {0070428077, 9780070428072},
 edition = {1},
 publisher = {McGraw-Hill, Inc.},
 address = {New York, NY, USA},
} 

@book{BOOK:9,
Abstract = {Your no-nonsense guide to making sense of machine learning Machine learning can be a mind-boggling concept for the masses, but those who are in the trenches of computer programming know just how invaluable it is. Without machine learning, fraud detection, web search results, real-time ads on web pages, credit scoring, automation, and email spam filtering wouldn't be possible, and this is only showcasing just a few of its capabilities. Written by two data science experts, Machine Learning For Dummies offers a much-needed entry point for anyone looking to use machine learning to accomplish practical tasks. Covering the entry-level topics needed to get you familiar with the basic concepts of machine learning, this guide quickly helps you make sense of the programming languages and tools you need to turn machine learning-based tasks into a reality. Whether you're maddened by the math behind machine learning, apprehensive about AI, perplexed by preprocessing data—or anything in between—thi},
Author = {Mueller, John and Massaron, Luca},
ISBN = {9781119245513},
Publisher = {For Dummies},
Series = {For Dummies},
Title = {Machine Learning For Dummies.},
URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=nlebk&AN=1237397&lang=sv&site=eds-live&scope=site},
Year = {2016},
}

@book{BOOK:10,
  title={Introduction to Neural Networks with Java},
  author={Heaton, J.},
  isbn={9781604390087},
  url={https://books.google.se/books?id=Swlcw7M4uD8C},
  year={2008},
  publisher={Heaton Research}
}

@book{BOOK:11,
 author = {He, Haibo and Ma, Yunqian},
 title = {Imbalanced Learning: Foundations, Algorithms, and Applications},
 year = {2013},
 isbn = {1118074629, 9781118074626},
 edition = {1st},
 publisher = {Wiley-IEEE Press},
} 

@book{BOOK:12,
 author = {Aggarwal, Charu C.},
 title = {Data Mining: The Textbook},
 year = {2015},
 isbn = {3319141414, 9783319141411},
 publisher = {Springer Publishing Company, Incorporated},
} 

@book{BOOK:13,
 author = {Beysolow,II, Taweh},
 title = {Introduction to Deep Learning Using R: A Step-by-Step Guide to Learning and Implementing Deep Learning Models Using R},
 year = {2017},
 isbn = {1484227336, 9781484227336},
 edition = {1st},
 publisher = {Apress},
 address = {Berkely, CA, USA},
}

@book{BOOK:14,
 author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
 title = {An Introduction to Statistical Learning: With Applications in R},
 year = {2014},
 isbn = {1461471370, 9781461471370},
 publisher = {Springer Publishing Company, Incorporated},
} 

@book{BOOK:15,
 author = {Pyle, Dorian},
 title = {Data Preparation for Data Mining},
 year = {1999},
 isbn = {1558605290, 9781558605299},
 edition = {1st},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
}

@book{BOOK:16,
Author = {Refaat, Mamdouh},
ISBN = {9780123735775},
Publisher = {Morgan Kaufmann},
Series = {The Morgan Kaufmann Series in Data Management Systems},
Title = {Data Preparation for Data Mining Using SAS.},
URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=nlebk&AN=195005&lang=sv&site=eds-live&scope=site},
Year = {2007},
}


@book{BOOK:17,
  added-at = {2016-11-02T09:32:16.000+0100},
  address = {Amsterdam},
  author = {Witten, Ian H. and Frank, Eibe and Hall, Mark A.},
  biburl = {https://www.bibsonomy.org/bibtex/2f317b38c79737297cecf9b9bd8ef2f3f/flint63},
  edition = 3,
  file = {ACM Learning Center eBook:2011/WittenFrankHall11.pdf:PDF;Amazon Search inside:http\://www.amazon.de/gp/reader/0123748569/:URL;Related Web Site:http\://www.cs.waikato.ac.nz/ml/weka/:URL},
  groups = {public},
  interhash = {2626c7da45fec4b481e2e7fca4f43bd3},
  intrahash = {f317b38c79737297cecf9b9bd8ef2f3f},
  isbn = {978-0-12-374856-0},
  keywords = {01801 105 book shelf elsevier ai learn data pattern recognition analysis tool java},
  publisher = {Morgan Kaufmann},
  series = {Morgan Kaufmann Series in Data Management Systems},
  timestamp = {2018-04-16T11:32:52.000+0200},
  title = {Data Mining: Practical Machine Learning Tools and Techniques},
  url = {http://www.sciencedirect.com/science/book/9780123748560},
  username = {flint63},
  year = 2011
}




@ARTICLE{ARTICLE:1,
	author = "A.K. Arvidsson",
    	title = "The Winter Model – A new way to calculate socio-economic costs depending on winter maintenance strategy",
    	journal = "Cold Regions Science and Technology
Volume 136, April 2017, Pages 30-36",
    	year = "2017",
}

@ARTICLE{ARTICLE:2,
	author = "A.K. Arvidsson",
    	title = "The Winter Model – A new way to calculate socio-economic costs depending on winter maintenance strategy",
    	journal = "Cold Regions Science and Technology
Volume 136, April 2017, Pages 30-36",
    	year = "2017",
}

@article{ARTICLE:3,
	Author = {Domingos, Pedro},
	ISSN = {00010782},
	Journal = {Communications of the ACM},
	Keywords = {MACHINE theory, COMPUTER science, ALGORITHMS, ANALYSIS of variance, DATA mining, COMPUTER programming, BAYESIAN analysis, 		DECISION trees, SCALABILITY (Systems engineering), MACHINE learning, COMPUTATIONAL learning theory, GENERALIZATION},
	Number = {10},
	Pages = {78 - 87},
	Title = {A Few Useful Things to Know About Machine Learning.},
	Volume = {55},
	URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=buh&AN=82151052&lang=sv&site=eds-		live&scope=site},
	Year = {2012},
}

@article{ARTICLE:4,
	Abstract = {Machine learning has become a pivotal tool for many projects in computational biology, bioinformatics, and health informatics. Nevertheless, beginners and biomedical researchers often do not have enough experience to run a data mining project effectively, and therefore can follow incorrect practices, that may lead to common mistakes or over-optimistic results. With this review, we present ten quick tips to take advantage of machine learning in any computational biology context, by avoiding some common errors that we observed hundreds of times in multiple bioinformatics projects. We believe our ten suggestions can strongly help any machine learning practitioner to carry on a successful project in computational biology and related sciences. [ABSTRACT FROM AUTHOR]},
Author = {Chicco, Davide},
ISSN = {17560381},
Journal = {BioData Mining},
Keywords = {MACHINE learning, BIOINFORMATICS, DATA mining, COMPUTATIONAL biology, COMPUTATIONAL neuroscience, Bioinformatics, Biomedical informatics, Computational biology, Computational intelligence, Data mining, Health informatics, Machine learning, Tips},
Pages = {1 - 17},
Title = {Ten quick tips for machine learning in computational biology.},
Volume = {10},
URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=aph&AN=126676440&lang=sv&site=eds-live&scope=site},
Year = {2017},
}

@ARTICLE{ARTICLE:5,
ISSN = {03067734},
Journal = {INTERNATIONAL STATISTICAL REVIEW},
Keywords = {REGRESSION, VARIABLE SELECTION, CROSS-VALIDATION, BOOTSTRAP, PREDICTION ERROR, SUBSET SELECTION, Statistics & Probability, Mathematics},
Number = {3},
Pages = {291 - 319},
Title = {SUBMODEL SELECTION AND EVALUATION IN REGRESSION - THE X-RANDOM CASE.},
Volume = {60},
URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=edswsc&AN=A1992KC62300004&lang=sv&site=eds-live&scope=site},
Year = {n.d.},
}

@ARTICLE{ARTICLE:6,
Author = {Wolpert, D.H. ( 1 ) and Macready, W.G. ( 2 )},
ISSN = {1089778X},
Journal = {IEEE Transactions on Evolutionary Computation},
Keywords = {Evolutionary algorithms, Information theory, Optimization},
Number = {1},
Pages = {67-82},
Title = {No free lunch theorems for optimization.},
Volume = {1},
URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=edselc&AN=edselc.2-52.0-0031118203&lang=sv&site=eds-live&scope=site},
Year = {1997},
}

@article{ARTICLE:7,
Author = {Kotsiantis, S.B.},
ISSN = {03505596},
Journal = {Informatica (Ljubljana)},
Keywords = {Classifiers, Data mining techniques, Intelligent data analysis, Learning algorithms},
Number = {3},
Pages = {249-268},
Title = {Supervised machine learning: A review of classification techniques.},
Volume = {31},
URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=edselc&AN=edselc.2-52.0-36749047332&lang=sv&site=eds-live&scope=site},
Year = {2007},
}

@article{ARTICLE:8,
title = "An efficient algorithm for optimal pruning of decision trees",
journal = "Artificial Intelligence",
volume = "83",
number = "2",
pages = "347 - 362",
year = "1996",
issn = "0004-3702",
doi = "https://doi.org/10.1016/0004-3702(95)00060-7",
url = "http://www.sciencedirect.com/science/article/pii/0004370295000607",
author = "Hussein Almuallim"
}

@article{ARTICLE:9,
Author = {Wu, X. ( 1 ) and Kumar, V. ( 2 ) and Steinbach, M. ( 2 ) and Ross, Q.J. ( 3 ) and Ghosh, J. ( 4 ) and Yang, Q. ( 5 ) and Motoda, H. ( 6 ) and McLachlan, G.J. ( 7 ) and Ng, A. ( 8 ) and Liu, B. ( 9 ) and Yu, P.S. ( 10 ) and Zhou, Z.-H. ( 11 ) and Hand, D.J. ( 12 ) and Steinberg, D. ( 13 )},
ISSN = {02191377},
Journal = {Knowledge and Information Systems},
Number = {1},
Pages = {1-37},
Title = {Top 10 algorithms in data mining.},
Volume = {14},
URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=edselc&AN=edselc.2-52.0-37549018049&lang=sv&site=eds-live&scope=site},
Year = {2008},
}

@article{ARTICLE:10,
Author = {Bermingham, M.L. ( 1 ) and Spiliopoulou, A. ( 1 ) and Hayward, C. ( 1 ) and Wright, A.F. ( 1 ) and Navarro, P. ( 1 ) and Haley, C.S. ( 1,2 ) and Pong-Wong, R. ( 2 ) and Rudan, I. ( 3 ) and Campbell, H. ( 3 ) and Wilson, J.F. ( 3 ) and Agakov, F. ( 4 )},
ISSN = {20452322},
Journal = {Scientific Reports},
Title = {Application of high-dimensional feature selection: Evaluation for genomic prediction in man.},
Volume = {5},
URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=edselc&AN=edselc.2-52.0-84930221871&lang=sv&site=eds-live&scope=site},
Year = {2015},
}

@article{ARTICLE:11,
Author = {Hyafil, L. ( 1 ) and Rivest, R.L. ( 2 )},
ISSN = {00200190},
Journal = {Information Processing Letters},
Keywords = {Binary decision trees, computational complexity, NP-complete},
Number = {1},
Pages = {15-17},
Title = {Constructing optimal binary decision trees is NP-complete.},
Volume = {5},
URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=edselc&AN=edselc.2-52.0-0001815269&lang=sv&site=eds-live&scope=site},
Year = {1976},
}

@article{ARTICLE:12,
title = "Effects of domain characteristics on instance-based learning algorithms",
journal = "Theoretical Computer Science",
volume = "298",
number = "1",
pages = "207 - 233",
year = "2003",
note = "Selected Papers in honour of Setsuo Arikawa",
issn = "0304-3975",
doi = "https://doi.org/10.1016/S0304-3975(02)00424-3",
url = "http://www.sciencedirect.com/science/article/pii/S0304397502004243",
author = "Seishi Okamoto and Nobuhiro Yugami",
keywords = "Instance-based learning, -Nearest neighbor classifier, Average-case analysis, Expected accuracy, Optimal value of "
}

@Article{ARTICLE:13,
author="Domingos, Pedro
and Pazzani, Michael",
title="On the Optimality of the Simple Bayesian Classifier under Zero-One Loss",
journal="Machine Learning",
year="1997",
month="Nov",
day="01",
volume="29",
number="2",
pages="103--130",
abstract="The simple Bayesian classifier is known to be optimal when attributes are independent given the class, but the question of whether other sufficient conditions for its optimality exist has so far not been explored. Empirical results showing that it performs surprisingly well in many domains containing clear attribute dependences suggest that the answer to this question may be positive. This article shows that, although the Bayesian classifier's probability estimates are only optimal under quadratic loss if the independence assumption holds, the classifier itself can be optimal under zero-one loss (misclassification rate) even when this assumption is violated by a wide margin. The region of quadratic-loss optimality of the Bayesian classifier is in fact a second-order infinitesimal fraction of the region of zero-one optimality. This implies that the Bayesian classifier has a much greater range of applicability than previously thought. For example, in this article it is shown to be optimal for learning conjunctions and disjunctions, even though they violate the independence assumption. Further, studies in artificial domains show that it will often outperform more powerful classifiers for common training set sizes and numbers of attributes, even if its bias is a priori much less appropriate to the domain. This article's results also imply that detecting attribute dependence is not necessarily the best way to extend the Bayesian classifier, and this is also verified empirically.",
issn="1573-0565",
doi="10.1023/A:1007413511361",
url="https://doi.org/10.1023/A:1007413511361"
}

@Article{ARTICLE:14,
author="Frank, Eibe
and Trigg, Leonard
and Holmes, Geoffrey
and Witten, Ian H.",
title="Technical Note: Naive Bayes for Regression",
journal="Machine Learning",
year="2000",
month="Oct",
day="01",
volume="41",
number="1",
pages="5--25",
abstract="Despite its simplicity, the naive Bayes learning scheme performs well on most classification tasks, and is often significantly more accurate than more sophisticated methods. Although the probability estimates that it produces can be inaccurate, it often assigns maximum probability to the correct class. This suggests that its good performance might be restricted to situations where the output is categorical. It is therefore interesting to see how it performs in domains where the predicted value is numeric, because in this case, predictions are more sensitive to inaccurate probability estimates.",
issn="1573-0565",
doi="10.1023/A:1007670802811",
url="https://doi.org/10.1023/A:1007670802811"
}

@article{ARTICLE:15,
Abstract = {Graphical models are frequently used to explore networks, such as genetic networks, among a set of variables. This is usually carried out via exploring the sparsity of the precision matrix of the variables under consideration. Penalized likelihood methods are often used in such explorations. Yet, positive-definiteness constraints of precision matrices make the optimization problem challenging. We introduce nonconcave penalties and the adaptive LASSO penalty to attenuate the bias problem in the network estimation. Through the local linear approximation to the nonconcave penalty functions, the problem of precision matrix estimation is recast as a sequence of penalized likelihood problems with a weighted L(1) penalty and solved using the efficient algorithm of Friedman et al. [Biostatistics 9 (2008) 432-441]. Our estimation schemes are applied to two real datasets. Simulation experiments and asymptotic theory are used to justify our proposed methods.},
Author = {Feng, Yang and Fan, Jianqing and Feng, Yang and Wu, Yichao},
ISSN = {19326157},
Journal = {ANNALS OF APPLIED STATISTICS},
Keywords = {Adaptive LASSO, covariance selection, Gaussian concentration graphical model, genetic network, LASSO, precision matrix, SCAD, Statistics & Probability, Mathematics},
Number = {2},
Pages = {521 - 541},
Title = {NETWORK EXPLORATION VIA THE ADAPTIVE LASSO AND SCAD PENALTIES.},
Volume = {3},
URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=edswsc&AN=000271979600002&lang=sv&site=eds-live&scope=site},
Year = {n.d.},
}

@article{ARTICLE:16,
Author = {CHOWDHURY, ALOK KUMAR and TJONDRONEGORO, DIAN and CHANDRAN, VINOD and TROST, STEWART G.},
ISSN = {01959131},
Journal = {Medicine and Science in Sports and Exercise},
Keywords = {*CLASSIFICATION, *WRIST, *MOTION capture (Medicine), *PHYSICAL activity, ALGORITHMS, DECISION trees, ACCELEROMETRY, BAGGING, BOOSTED DECISION TREES, MACHINE LEARNING, MOTION SENSORS, PATTERN RECOGNITION, RANDOM FOREST},
Number = {9},
Pages = {1965 - 1973},
Title = {Ensemble Methods for Classification of Physical Activities from Wrist Accelerometry.},
Volume = {49},
URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=s3h&AN=124667245&lang=sv&site=eds-live&scope=site},
Year = {2017},
}

@article{ARTICLE:17,
  added-at = {2013-01-10T07:09:13.000+0100},
  author = {Aly, M.},
  biburl = {https://www.bibsonomy.org/bibtex/29184db6dce2d01c92a664db199a14118/lychen1109},
  interhash = {5db3682afcaaaacba2c00479f92c4c6c},
  intrahash = {9184db6dce2d01c92a664db199a14118},
  journal = {Neural networks},
  keywords = {classification multiclass},
  pages = {1-9},
  timestamp = {2013-01-10T07:09:13.000+0100},
  title = {Survey on multiclass classification methods},
  year = 2005
}

@article{ARTICLE:18,
Abstract = {Datasets for training object recognition systems are steadily increasing in size. This paper investigates the question of whether existing detectors will continue to improve as data grows, or saturate in performance due to limited model complexity and the Bayes risk associated with the feature spaces in which they operate. We focus on the popular paradigm of discriminatively trained templates defined on oriented gradient features. We investigate the performance of mixtures of templates as the number of mixture components and the amount of training data grows. Surprisingly, even with proper treatment of regularization and 'outliers', the performance of classic mixture models appears to saturate quickly ( $${\sim }10$$ templates and $${\sim }100$$ positive training examples per template). This is not a limitation of the feature space as compositional mixtures that share template parameters via parts and that can synthesize new templates not encountered during training yield significantl},
Author = {Zhu, Xiangxin and Vondrick, Carl and Fowlkes, Charless and Ramanan, Deva},
ISSN = {09205691},
Journal = {International Journal of Computer Vision},
Keywords = {OBJECT recognition (Computer vision), BIG data, DESIGN templates, OUTLIERS (Statistics), DATA editing, Mixture models, Object detection, Part models},
Number = {1},
Pages = {76 - 92},
Title = {Do We Need More Training Data?.},
Volume = {119},
URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=aph&AN=116285795&lang=sv&site=eds-live&scope=site},
Year = {2016},
}

@article{ARTICLE:19,
Abstract = {Background: Multi-label classification of data remains to be a challenging problem. Because of the complexity of the data, it is sometimes difficult to infer information about classes that are not mutually exclusive. For medical data, patients could have symptoms of multiple different diseases at the same time and it is important to develop tools that help to identify problems early. Intelligent health risk prediction models built with deep learning architectures offer a powerful tool for physicians to identify patterns in patient data that indicate risks associated with certain types of chronic diseases. Results: Physical examination records of 110,300 anonymous patients were used to predict diabetes, hypertension, fatty liver, a combination of these three chronic diseases, and the absence of disease (8 classes in total). The dataset was split into training (90%) and testing (10%) sub-datasets. Ten-fold cross validation was used to evaluate prediction accuracy with metrics such as pr},
Author = {Maxwell, Andrew and Runzhi, Li and Bei, Yang and Heng, Weng and Aihua, Ou and Huixiao, Hong and Zhaoxian, Zhou and Ping, Gong and Chaoyang, Zhang},
ISSN = {14712105},
Journal = {BMC Bioinformatics},
Keywords = {HEALTH risk assessment, DEEP learning (Machine learning), SUPPORT vector machines, CHRONIC diseases, NEURAL networks (Computer science), Deep learning, Deep neural networks, Intelligent health risk prediction, Medical health records, Multi-label classification},
Pages = {121 - 131},
Title = {Deep learning architectures for multi-label classification of intelligent health risk prediction.},
Volume = {18},
URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=aph&AN=127122779&lang=sv&site=eds-live&scope=site},
Year = {2017},
}

@article {ARTICLE:20,
	author = {Skocik, Michael and Collins, John and Callahan-Flintoft, Chloe and Bowman, Howard and Wyble, Brad},
	title = {I TRIED A BUNCH OF THINGS: THE DANGERS OF UNEXPECTED OVERFITTING IN CLASSIFICATION},
	year = {2016},
	doi = {10.1101/078816},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Machine learning is a powerful set of techniques that has enhanced the abilities of neuroscientists to interpret information collected through EEG, fMRI, MEG, and PET data. With these new techniques come new dangers of overfitting that are not well understood by the neuroscience community. In this article, we use Support Vector Machine (SVM) classifiers, and genetic algorithms to demonstrate the ease by which overfitting can occur, despite the use of cross validation. We demonstrate that comparable and non-generalizable results can be obtained on informative and non-informative (i.e. random) data by iteratively modifying hyperparameters in seemingly innocuous ways. We recommend a number of techniques for limiting overfitting, such as lock boxes, blind analyses, and pre-registrations. These techniques, although uncommon in neuroscience applications, are common in many other fields that use machine learning, including computer science and physics. Adopting similar safeguards is critical for ensuring the robustness of machine-learning techniques.},
	URL = {https://www.biorxiv.org/content/early/2016/10/03/078816},
	eprint = {https://www.biorxiv.org/content/early/2016/10/03/078816.full.pdf},
	journal = {bioRxiv}
}

@article{ARTICLE:21,
Abstract = {This paper aims to introduce penalized estimation techniques in clinical investigations of diabetes, as well as to assess their possible advantages and limitations. Data from a previous study was used to carry out the simulations to assess: a) which procedure results in the lowest prediction error of the final model in the setting of a large number of predictor variables with high multicollinearity (of importance if insulin sensitivity should be predicted) and b) which procedure achieves the most accurate estimate of regression coefficients in the setting of fewer predictors with small unidirectional effects and moderate correlation between explanatory variables (of importance if the specific relation between an independent variable and insulin sensitivity should be examined). Moreover a special focus is on the correct direction of estimated parameter effects, a non-negligible source of error and misinterpretation of study results. The simulations were performed for varying sample siz},
Author = {Göbl, Christian S. and Bozkurt, Latife and Tura, Andrea and Pacini, Giovanni and Kautzky-Willer, Alexandra and Mittlböck, Martina},
ISSN = {19326203},
Journal = {PLoS ONE},
Keywords = {TREATMENT of diabetes, INSULIN resistance, REGRESSION analysis, METABOLIC disorders, COMPARATIVE studies, PERFORMANCE evaluation, Research Article},
Number = {10},
Pages = {1 - 19},
Title = {Application of Penalized Regression Techniques in Modelling Insulin Sensitivity by Correlated Metabolic Parameters.},
Volume = {10},
URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=aph&AN=110795719&lang=sv&site=eds-live&scope=site},
Year = {2015},
}

@article{ARTICLE:22,
title = "Performance Analysis of Classifier Models to Predict Diabetes Mellitus",
journal = "Procedia Computer Science",
volume = "47",
pages = "45 - 51",
year = "2015",
note = "Graph Algorithms, High Performance Implementations and Its Applications ( ICGHIA 2014 )",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2015.03.182",
url = "http://www.sciencedirect.com/science/article/pii/S1877050915004500",
author = "J. Pradeep Kandhasamy and S. Balamurali",
keywords = "J48Decision Tree, KNN algorithm, Random Forest, Support Vector Machines, Diabetes Mellitus;"
}

@ARTICLE{ARTICLE:23, 
author={A. Sen and M. M. Islam and K. Murase and X. Yao}, 
journal={IEEE Transactions on Cybernetics}, 
title={Binarization With Boosting and Oversampling for Multiclass Classification}, 
year={2016}, 
volume={46}, 
number={5}, 
pages={1078-1091}, 
keywords={learning (artificial intelligence);neural nets;pattern classification;support vector machines;C4.5;OVA binarization technique;base classifiers;binarization-with-boosting-and-oversampling;binary classifiers;decision boundaries;error reduction;global consistency;k-nearest neighbor;local consistency;multiclass classification problems;multiclass semisupervised classification problem;multiclass supervised classification problem;neural networks;one-versus-all binarization technique;random forest;repeated incremental pruning;support vector machine;Boosting;Classification algorithms;Cybernetics;Support vector machines;Testing;Time complexity;Training;Binarization;boosting;multiclass classification;oversampling}, 
doi={10.1109/TCYB.2015.2423295}, 
ISSN={2168-2267}, 
month={May},}

@article{ARTICLE:24,
author  = {Guillaume  Lema{{\^i}}tre and Fernando Nogueira and Christos K. Aridas},
title   = {Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning},
journal = {Journal of Machine Learning Research},
year    = {2017},
volume  = {18},
number  = {17},
pages   = {1-5},
url     = {http://jmlr.org/papers/v18/16-365}
}

@article{ARTICLE:25,
Abstract = {Most machine learning researchers perform quantitative experiments to estimate generalization error and compare the performance of different algorithms ( in particular, their proposed algorithm). In order to be able to draw statistically convincing conclusions, it is important to estimate the uncertainty of such estimates. This paper studies the very commonly used K-fold cross-validation estimator of generalization performance. The main theorem shows that there exists no universal ( valid under all distributions) unbiased estimator of the variance of K-fold cross-validation. The analysis that accompanies this result is based on the eigen-decomposition of the covariance matrix of errors, which has only three different eigenvalues corresponding to three degrees of freedom of the matrix and three components of the total variance. This analysis helps to better understand the nature of the problem and how it can make naive estimators ( that don't take into account the error correlations du},
ISSN = {15324435},
Journal = {JOURNAL OF MACHINE LEARNING RESEARCH},
Keywords = {cross-validation, variance estimators, k-fold cross-validation, statistical comparisons of algorithms, Automation & Control Systems, Computer Science, Artificial Intelligence, Automation & Control Systems, Computer Science},
Pages = {1089 - 1105},
Title = {No unbiased estimator of the variance of K-fold cross-validation.},
Volume = {5},
URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=edswsc&AN=000236328100001&lang=sv&site=eds-live&scope=site},
Year = {n.d.},
}

@INPROCEEDINGS{IP:1, 
	author={S. Bhatia and J. Malhotra}, 	
	booktitle={2014 International Conference on Advances in Engineering Technology Research (ICAETR - 2014)}, 	
	title={A survey on impact of lines of code on software complexity}, 
	year={2014}, 
	volume={}, 
	number={}, 
	pages={1-4}, 
	keywords={computational complexity;software metrics;source code (software);SLOC;lines of code;problem complexity;programming productivity;project size;software complexity;software industry;software system;source code;Biological system modeling;Complexity theory;Estimation;Size 			measurement;Software;Software measurement;Complexity;Effort;Lines of code component}, 	
	doi={10.1109/ICAETR.2014.7012875}, 
	ISSN={2347-9337}, 
	month={Aug},
}

@INPROCEEDINGS{IP:2,
    author = {Ron Kohavi},
    title = {A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection},
    booktitle = {},
    year = {1995},
    pages = {1137--1143},
    publisher = {Morgan Kaufmann}
}

@INPROCEEDINGS{IP:3, 
author={L. Zhong and L. Lin and Z. Lu and Y. Wu and Z. Lu and M. Huang and W. Yang and Q. Feng}, 
booktitle={2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)}, 
title={Predict CT image from MRI data using KNN-regression with learned local descriptors}, 
year={2016}, 
volume={}, 
number={}, 
pages={743-746}, 
keywords={biomedical MRI;computerised tomography;learning (artificial intelligence);medical image processing;radiation therapy;regression analysis;CT image prediction;KNN regression;MR based radiation therapy;MRI data;PET-MR hybrid imaging systems;attenuation correction;dose planning;learned local descriptors;low rank approximation;manifold regularization;supervised descriptor learning;Attenuation;Computed tomography;Learning systems;Magnetic resonance imaging;Manifolds;Training;CT prediction;PET attenuation correction;local descriptor learning;low-rank approximation}, 
doi={10.1109/ISBI.2016.7493373}, 
ISSN={}, 
month={April},}

@INPROCEEDINGS{IP:4,
    author = {Harry Zhang},
    title = {The Optimality of Naïve Bayes},
    booktitle = {In FLAIRS2004 conference},
    year = {2004}
}

@inproceedings{IP:5,
 author = {Caruana, Rich and Niculescu-Mizil, Alexandru},
 title = {An Empirical Comparison of Supervised Learning Algorithms},
 booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
 series = {ICML '06},
 year = {2006},
 isbn = {1-59593-383-2},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {161--168},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1143844.1143865},
 doi = {10.1145/1143844.1143865},
 acmid = {1143865},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@INPROCEEDINGS{IP:6, 
author={S. S. Patil and S. P. Sonavane}, 
booktitle={2017 IEEE Third International Conference on Big Data Computing Service and Applications (BigDataService)}, 
title={Enriched Over_Sampling Techniques for Improving Classification of Imbalanced Big Data}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-10}, 
keywords={Big Data;learning (artificial intelligence);pattern classification;pattern clustering;Apache Hadoop;F measures;MapReduce environment;NoSQL Big Data;ROC area;UCI/KEEL repository;balanced class distribution;binary/multiclass imbalanced data sets;classifiers;cluster based advanced approach;data management options;enriched oversampling techniques;equal misclassification costs;imbalanced big data classification;imbalanced class distribution;imbalanced data set handling;multiclass imbalance problem;nonclustered based advanced approach;standard classifier learning algorithms;two-class problem;Area measurement;Big Data;Cancer;Classification algorithms;Data mining;Machine learning algorithms;Standards;Big Data;data level approach;imbalanced data sets;over_sampling techniques}, 
doi={10.1109/BigDataService.2017.19}, 
ISSN={}, 
month={April},
}

@inproceedings{IP:7,
Address = {(1)Microsoft Research},
Author = {Dwork, C. ( 1 ) and Pitassi, T. ( 2 ) and Feldman, V. ( 3 ) and Reingold, O. ( 4 ) and Hardt, M. ( 5 ) and Roth, A. ( 6 )},
Booktitle = {Advances in Neural Information Processing Systems},
Edition = {2015-January},
Number = {Advances in Neural Information Processing Systems 28 - Proceedings of the 2015 Conference},
Pages = {2350-2358},
Title = {Generalization in adaptive data analysis and holdout reuse.},
Volume = {2015-January},
URL = {http://proxy.lib.ltu.se/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=edselc&AN=edselc.2-52.0-84965181547&lang=sv&site=eds-live&scope=site},
Year = {2015},
}

@ELECTRONIC{WEBSITE:1,
	url = "https://machinelearningmastery.com/classification-versus-regression-in-machine-learning/",
	author = "Jason Brownlee",
	title = "Difference Between Classification and Regression in Machine Learning",
	year = "2017",
	urldate = "2018-01-29", 
}

@ELECTRONIC{WEBSITE:2,
	url = "https://www.trafikverket.se/resa-och-trafik/underhall-av-vag-och-jarnvag/Sa-skoter-vi-vagar/Vintervaghallning/",
	author = "Trafikverket",
	title = "Vinterväghållning",
	year = "2017",
	urldate = "2018-02-07", 
}

@ELECTRONIC{WEBSITE:3,
	url = "https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/",
	author = "J. Brownlee",
	title = "Supervised and Unsupervised Machine Learning Algorithms",
	year = "2016",
	urldate = "2018-02-28", 
}

@ELECTRONIC{WEBSITE:4,
	url = "https://techcrunch.com/2017/05/31/google-says-its-machine-learning-tech-now-blocks-99-9-of-gmail-spam-and-phishing-messages/",
	author = "F. Lardinois",
	title = "Google says its machine learning tech now blocks 99.9\% of Gmail spam and phishing messages",
	year = "2017",
	urldate = "2018-03-19", 
}

@ELECTRONIC{WEBSITE:5, 
	url = "https://archive.ics.uci.edu/ml/datasets/iris",
	author = "R.A. Fisher",
	title = "Iris Data Set",
	urldate = "2018-03-19"
}

@ELECTRONIC{WEBSITE:6, 
	url = "https://machinelearningmastery.com/difference-test-validation-datasets/",
	author = "J. Brownlee",
	title = "What is the Difference Between Test and Validation Datasets?",
	urldate = "2018-03-20"
}

@ELECTRONIC{WEBSITE:7, 
	url = "https://www.quora.com/What-are-hyperparameters-in-machine-learning",
	author = "X. Amatriain",
	title = "What are hyperparameters in machine learning?",
	urldate = "2018-03-20"
}

@ELECTRONIC{WEBSITE:8, 
	url = "https://machinelearningmastery.com/classification-versus-regression-in-machine-learning/",
	author = "J. Brownlee",
	title = "Difference Between Classification and Regression in Machine Learning",
	urldate = "2018-03-22"
}

@ELECTRONIC{WEBSITE:9, 
	url = "https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/",
	author = "J. Brownlee",
	title = "Overfitting and Underfitting With Machine Learning Algorithms",
	urldate = "2018-03-22"
}

@ELECTRONIC{WEBSITE:10, 
	url = "http://scottclowe.com/2016-03-19-stratified-regression-partitions/",
	author = "S.C. Lowe",
	title = "Stratified Validation Splits for Regression Problems",
	urldate = "2018-03-29"
}

@ELECTRONIC{WEBSITE:11, 
	url = "https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/",
	author = "Jason Brownlee",
	title = "Overfitting and Underfitting With Machine Learning Algorithms",
	urldate = "2018-03-29"
}

@ELECTRONIC{WEBSITE:12, 
	url = "https://www.r-bloggers.com/machine-learning-explained-regularization/",
	author = "EnhanceDataScience",
	title = "Machine Learning Explained: Regularization",
	urldate = "2018-03-28"
}

@ELECTRONIC{WEBSITE:13, 
	url = "https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a",
	author = "Prashant Gupta",
	title = "Regularization in Machine Learning",
	urldate = "2018-04-03"
}

@ELECTRONIC{WEBSITE:14, 
	url = "https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/",
	author = "Jason Brownlee",
	title = "A tour of Machine Learning Algorithms",
	urldate = "2018-04-05"
}

@ELECTRONIC{WEBSITE:15, 
	url = "http://scikit-learn.org/stable/supervised_learning.html",
	author = "scikit-learn developers",
	title = "Supervised Learning",
	urldate = "2018-04-05"
}

@ELECTRONIC{WEBSITE:16, 
	url = "http://scikit-learn.org/stable/modules/tree.html",
	author = "scikit-learn developers",
	title = "Decision trees",
	urldate = "2018-04-10"
}

@ELECTRONIC{WEBSITE:17, 
	url = "http://scikit-learn.org/stable/modules/classes.html",
	author = "scikit-learn developers",
	title = "API Reference",
	urldate = "2018-04-10"
}

@ELECTRONIC{WEBSITE:18, 
	url = "http://scikit-learn.org/stable/modules/multiclass.html",
	author = "scikit-learn developers",
	title = "Multiclass and multilabel algorithms",
	urldate = "2018-04-10"
}

@ELECTRONIC{WEBSITE:19, 
	url = "https://www.vaisala.com/en/products/instruments-sensors-and-other-measurement-devices/weather-stations-and-sensors/dst111",
	author = "Vaisala",
	title = "DST111 Remote Surface Temperature Sensor",
	urldate = "2018-04-17"
}

@ELECTRONIC{WEBSITE:20, 
	url = "https://docs.microsoft.com/en-us/machine-learning-server/r/how-to-choose-microsoftml-algorithms-cheatsheet",
	author = "Microsoft",
	title = "Cheat sheet: How to choose a MicrosoftML algorithm",
	urldate = "2018-04-18"
}

@ELECTRONIC{WEBSITE:21, 
	url = "http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html",
	author = "Scikit-learn",
	title = "Choosing the right estimator",
	urldate = "2018-04-18"
}

@ELECTRONIC{WEBSITE:22, 
	url = "https://github.com/scikit-learn-contrib/imbalanced-learn",
	author = "imbalanced-learn",
	title = "imbalanced-learn",
	urldate = "2018-04-25"
}

@ELECTRONIC{WEBSITE:23, 
	url = "http://contrib.scikit-learn.org/imbalanced-learn/stable/generated/imblearn.over\_sampling.SMOTE.html#imblearn.over\_sampling.SMOTE",
	author = "imbalanced-learn",
	title = "imblearn.over\_sampling.SMOTE",
	urldate = "2018-04-25"
}

@ELECTRONIC{WEBSITE:24, 
	url = "http://contrib.scikit-learn.org/imbalanced-learn/stable/over\_sampling.html",
	author = "imbalanced-learn",
	title = "Over-sampling",
	urldate = "2018-04-25"
}

@ELECTRONIC{WEBSITE:25, 
	url = "http://scikit-learn.org/stable/modules/model\_evaluation.html#from-binary-to-multiclass-and-multilabel",
	author = "Scikit-learn",
	title = "From binary to multiclass and multilabel",
	urldate = "2018-04-25"
}

@ELECTRONIC{WEBSITE:26, 
	url = "https://machinelearningmastery.com/why-you-should-be-spot-checking-algorithms-on-your-machine-learning-problems/",
	author = "Jason Brownlee",
	title = "Why you should be Spot-Checking Algorithms on your Machine Learning Problems",
	urldate = "2018-05-01"
}

@ELECTRONIC{WEBSITE:27, 
	url = "https://machinelearningmastery.com/a-gentle-introduction-to-scikit-learn-a-python-machine-learning-library/",
	author = "Jason Brownlee",
	title = "A Gentle Introduction to Scikit-Learn: A Python Machine Learning Library",
	urldate = "2018-05-01"
}

https://machinelearningmastery.com/a-gentle-introduction-to-scikit-learn-a-python-machine-learning-library/




https://docs.microsoft.com/en-us/machine-learning-server/r/how-to-choose-microsoftml-algorithms-cheatsheet






@MISC{MISC:1,
    author = {Mohamed Aly},
    title = {Survey on Multiclass Classification Methods},
    year = {2005}
}

@misc{MAIL:1,
  author        = "Victoria Moberg",
  howpublished  = "private communication",
  year          = "2018"
}

@MISC{REPORT:1,
  	title = "Trafikverkets årsredovisning 2015",
	author = "Trafikverket",
  	year = "2016",
}

@MISC{REPORT:2,
  	title = "Road weather information systems",
	author = "Trafikverket",
	year = "",
}

@MISC{REPORT:3,
  	title = "RFI RWIS asked questions",
	author = "Trafikverket",
	year = "2017",
}

@MISC{REPORT:4,
  	title = "Nästa generation VägVäderinformationsSystem (VViS)",
	author = "Trafikverket",
	year = "",
}

@MISC{REPORT:5,
  	title = "Slutrapport - Förstudie Indra 2020",
	author = "Trafikverket",
	year = "2016",
}








@ELECTRONIC{IMAGE:1,
	url = "https://commons.wikimedia.org/wiki/File:Rwis_station_Myggsjon_01.JPG",
	author = "Pelpet",
	title = "Rwis Station at sensor site Myggsjön",
	year = "2010",
	urldate = "2018-02-06", 
}





